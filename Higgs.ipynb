{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = pd.read_hdf('higgs_signal.h5')\n",
    "bkg = pd.read_hdf('higgs_bkg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton_pT</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet1_pt</th>\n",
       "      <th>jet1_eta</th>\n",
       "      <th>jet1_phi</th>\n",
       "      <th>jet1_btag</th>\n",
       "      <th>jet2_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>jet4_phi</th>\n",
       "      <th>jet4_btag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.723801</td>\n",
       "      <td>-0.914611</td>\n",
       "      <td>0.910944</td>\n",
       "      <td>1.194830</td>\n",
       "      <td>-0.448292</td>\n",
       "      <td>0.839489</td>\n",
       "      <td>-0.871428</td>\n",
       "      <td>0.587799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916982</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.867059</td>\n",
       "      <td>1.127180</td>\n",
       "      <td>1.211664</td>\n",
       "      <td>0.695883</td>\n",
       "      <td>0.694068</td>\n",
       "      <td>0.755813</td>\n",
       "      <td>0.761658</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.974119</td>\n",
       "      <td>0.660297</td>\n",
       "      <td>-1.362428</td>\n",
       "      <td>1.234102</td>\n",
       "      <td>1.677716</td>\n",
       "      <td>1.478815</td>\n",
       "      <td>0.408940</td>\n",
       "      <td>-0.105273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.017048</td>\n",
       "      <td>...</td>\n",
       "      <td>1.604089</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.938668</td>\n",
       "      <td>1.233898</td>\n",
       "      <td>0.990063</td>\n",
       "      <td>0.524871</td>\n",
       "      <td>0.900614</td>\n",
       "      <td>0.917613</td>\n",
       "      <td>1.083369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.946889</td>\n",
       "      <td>0.169416</td>\n",
       "      <td>1.210014</td>\n",
       "      <td>0.343294</td>\n",
       "      <td>-1.579545</td>\n",
       "      <td>0.999435</td>\n",
       "      <td>1.030804</td>\n",
       "      <td>-0.475041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558635</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>0.881641</td>\n",
       "      <td>0.845381</td>\n",
       "      <td>0.997408</td>\n",
       "      <td>0.695120</td>\n",
       "      <td>0.787132</td>\n",
       "      <td>0.657668</td>\n",
       "      <td>0.721147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.298084</td>\n",
       "      <td>-0.897079</td>\n",
       "      <td>1.224441</td>\n",
       "      <td>0.618091</td>\n",
       "      <td>0.856746</td>\n",
       "      <td>0.493122</td>\n",
       "      <td>-0.021810</td>\n",
       "      <td>-1.520042</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.973234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.848238</td>\n",
       "      <td>0.925814</td>\n",
       "      <td>0.973957</td>\n",
       "      <td>0.961469</td>\n",
       "      <td>0.946147</td>\n",
       "      <td>1.028120</td>\n",
       "      <td>0.848133</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.022289</td>\n",
       "      <td>-0.481195</td>\n",
       "      <td>0.169649</td>\n",
       "      <td>1.103255</td>\n",
       "      <td>0.744424</td>\n",
       "      <td>1.648197</td>\n",
       "      <td>-0.780327</td>\n",
       "      <td>-1.484007</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>0.675472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284934</td>\n",
       "      <td>1.550981</td>\n",
       "      <td>0.717778</td>\n",
       "      <td>0.752909</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>1.648921</td>\n",
       "      <td>1.138676</td>\n",
       "      <td>1.118826</td>\n",
       "      <td>0.977200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton_pT  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0   0.723801   -0.914611    0.910944                  1.194830   \n",
       "1   1.974119    0.660297   -1.362428                  1.234102   \n",
       "2   0.946889    0.169416    1.210014                  0.343294   \n",
       "3   1.298084   -0.897079    1.224441                  0.618091   \n",
       "4   1.022289   -0.481195    0.169649                  1.103255   \n",
       "\n",
       "   missing_energy_phi   jet1_pt  jet1_eta  jet1_phi  jet1_btag   jet2_pt  ...  \\\n",
       "0           -0.448292  0.839489 -0.871428  0.587799   0.000000  0.654446  ...   \n",
       "1            1.677716  1.478815  0.408940 -0.105273   0.000000  1.017048  ...   \n",
       "2           -1.579545  0.999435  1.030804 -0.475041   0.000000  0.435374  ...   \n",
       "3            0.856746  0.493122 -0.021810 -1.520042   2.173076  0.973234  ...   \n",
       "4            0.744424  1.648197 -0.780327 -1.484007   2.173076  0.675472  ...   \n",
       "\n",
       "   jet4_phi  jet4_btag      m_jj     m_jjj      m_lv     m_jlv      m_bb  \\\n",
       "0 -0.916982   3.101961  0.867059  1.127180  1.211664  0.695883  0.694068   \n",
       "1  1.604089   3.101961  1.938668  1.233898  0.990063  0.524871  0.900614   \n",
       "2  0.558635   3.101961  0.881641  0.845381  0.997408  0.695120  0.787132   \n",
       "3  0.581386   0.000000  0.848238  0.925814  0.973957  0.961469  0.946147   \n",
       "4 -0.284934   1.550981  0.717778  0.752909  0.996800  1.648921  1.138676   \n",
       "\n",
       "      m_wbb    m_wwbb  label  \n",
       "0  0.755813  0.761658      0  \n",
       "1  0.917613  1.083369      0  \n",
       "2  0.657668  0.721147      0  \n",
       "3  1.028120  0.848133      0  \n",
       "4  1.118826  0.977200      0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([sig, bkg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['m_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb', 'label']\n",
    "data_red = data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(['label'], axis=1).values\n",
    "labels = data[['label']].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = features.shape[0]\n",
    "num_feature_elements = features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_per_pop = 12 # Population size.\n",
    "num_parents_mating = 6 # Number of parents inside the mating pool.\n",
    "num_mutations = 3 # Number of elements to mutate.\n",
    "\n",
    "# Defining the population shape.\n",
    "pop_shape = (sol_per_pop, num_feature_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 28)\n"
     ]
    }
   ],
   "source": [
    "# Creating the initial population.\n",
    "new_population = np.random.randint(low=0, high=2, size=pop_shape)\n",
    "print(new_population.shape)\n",
    "\n",
    "best_outputs = []\n",
    "num_generations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ga' from '/eos/home-l/llayer/Higgs/ga.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation :  0\n",
      "0.7649215609763802\n",
      "0.7243597508310788\n",
      "0.7077711419219794\n",
      "0.7361910134657782\n",
      "0.7532368202172455\n",
      "0.6846897910114087\n",
      "0.6340605826400367\n",
      "0.6978888360432611\n",
      "0.6994038749916316\n",
      "0.6489120509673887\n",
      "0.7299963201933226\n",
      "0.712091218431102\n",
      "[0.76492156 0.72435975 0.70777114 0.73619101 0.75323682 0.68468979\n",
      " 0.63406058 0.69788884 0.69940387 0.64891205 0.72999632 0.71209122]\n",
      "Best result :  0.7649215609763802\n",
      "Generation :  1\n",
      "0.7649215609763802\n",
      "0.7532368202172455\n",
      "0.7361910134657782\n",
      "0.7299963201933226\n",
      "0.7243597508310788\n",
      "0.712091218431102\n",
      "0.7604326468187305\n",
      "0.730922654357421\n",
      "0.7470339995953774\n",
      "0.7099249813507177\n",
      "0.7266813258948648\n",
      "0.7515285156548821\n",
      "[0.76492156 0.75323682 0.73619101 0.72999632 0.72435975 0.71209122\n",
      " 0.76043265 0.73092265 0.747034   0.70992498 0.72668133 0.75152852]\n",
      "Best result :  0.7649215609763802\n",
      "Generation :  2\n",
      "0.7649215609763802\n",
      "0.7604326468187305\n",
      "0.7532368202172455\n",
      "0.7515285156548821\n",
      "0.7470339995953774\n",
      "0.7361910134657782\n",
      "0.7619016307870975\n",
      "0.7622812744308354\n",
      "0.7571398305415505\n",
      "0.7343101519593938\n",
      "0.7350445061918923\n",
      "0.7659730195876467\n",
      "[0.76492156 0.76043265 0.75323682 0.75152852 0.747034   0.73619101\n",
      " 0.76190163 0.76228127 0.75713983 0.73431015 0.73504451 0.76597302]\n",
      "Best result :  0.7659730195876467\n",
      "Generation :  3\n",
      "0.7659730195876467\n",
      "0.7649215609763802\n",
      "0.7622812744308354\n",
      "0.7619016307870975\n",
      "0.7604326468187305\n",
      "0.7571398305415505\n",
      "0.7746732782577608\n",
      "0.775220198364263\n",
      "0.773699327927896\n",
      "0.7746125297646954\n",
      "0.7764893964722162\n",
      "0.7678677021797551\n",
      "[0.76597302 0.76492156 0.76228127 0.76190163 0.76043265 0.75713983\n",
      " 0.77467328 0.7752202  0.77369933 0.77461253 0.7764894  0.7678677 ]\n",
      "Best result :  0.7764893964722162\n",
      "Generation :  4\n",
      "0.7764893964722162\n",
      "0.775220198364263\n",
      "0.7746732782577608\n",
      "0.7746125297646954\n",
      "0.773699327927896\n",
      "0.7678677021797551\n",
      "0.7732966338355423\n",
      "0.7760558919196606\n",
      "0.7730916134110999\n",
      "0.7738647217842993\n",
      "0.7769416352539255\n",
      "0.7674260243605588\n",
      "[0.7764894  0.7752202  0.77467328 0.77461253 0.77369933 0.7678677\n",
      " 0.77329663 0.77605589 0.77309161 0.77386472 0.77694164 0.76742602]\n",
      "Best result :  0.7769416352539255\n",
      "Generation :  5\n",
      "0.7769416352539255\n",
      "0.7764893964722162\n",
      "0.7760558919196606\n",
      "0.775220198364263\n",
      "0.7746732782577608\n",
      "0.7746125297646954\n",
      "0.7256838659441008\n",
      "0.7223649686812361\n",
      "0.7220257321984186\n",
      "0.7218421092023727\n",
      "0.7168016372981605\n",
      "0.7247563379320663\n",
      "[0.77694164 0.7764894  0.77605589 0.7752202  0.77467328 0.77461253\n",
      " 0.72568387 0.72236497 0.72202573 0.72184211 0.71680164 0.72475634]\n",
      "Best result :  0.7769416352539255\n",
      "Generation :  6\n",
      "0.7769416352539255\n",
      "0.7764893964722162\n",
      "0.7760558919196606\n",
      "0.775220198364263\n",
      "0.7746732782577608\n",
      "0.7746125297646954\n",
      "0.7594087844616833\n",
      "0.7599065731338848\n",
      "0.757562682297136\n",
      "0.7606316520862629\n",
      "0.7559407480412399\n",
      "0.758886338237875\n",
      "[0.77694164 0.7764894  0.77605589 0.7752202  0.77467328 0.77461253\n",
      " 0.75940878 0.75990657 0.75756268 0.76063165 0.75594075 0.75888634]\n",
      "Best result :  0.7769416352539255\n",
      "Generation :  7\n",
      "0.7769416352539255\n",
      "0.7764893964722162\n",
      "0.7760558919196606\n",
      "0.775220198364263\n",
      "0.7746732782577608\n",
      "0.7746125297646954\n",
      "0.7573324073971186\n",
      "0.7571532842894478\n",
      "0.7547003400354426\n",
      "0.7566578833131191\n",
      "0.7533217670896346\n",
      "0.757563967979529\n",
      "[0.77694164 0.7764894  0.77605589 0.7752202  0.77467328 0.77461253\n",
      " 0.75733241 0.75715328 0.75470034 0.75665788 0.75332177 0.75756397]\n",
      "Best result :  0.7769416352539255\n",
      "Generation :  8\n",
      "0.7769416352539255\n",
      "0.7764893964722162\n",
      "0.7760558919196606\n",
      "0.775220198364263\n",
      "0.7746732782577608\n",
      "0.7746125297646954\n",
      "0.7766587851274833\n",
      "0.7747849948742598\n",
      "0.7744852012905681\n",
      "0.775920160592751\n",
      "0.7734560584522644\n",
      "0.7762694988657985\n",
      "[0.77694164 0.7764894  0.77605589 0.7752202  0.77467328 0.77461253\n",
      " 0.77665879 0.77478499 0.7744852  0.77592016 0.77345606 0.7762695 ]\n",
      "Best result :  0.7769416352539255\n",
      "Generation :  9\n",
      "0.7769416352539255\n",
      "0.7766587851274833\n",
      "0.7764893964722162\n",
      "0.7762694988657985\n",
      "0.7760558919196606\n",
      "0.775920160592751\n",
      "0.7767859758499256\n",
      "0.7755876739424964\n",
      "0.7769290080161378\n",
      "0.7761816132907968\n",
      "0.7765678690154125\n",
      "0.7770822797242652\n",
      "[0.77694164 0.77665879 0.7764894  0.7762695  0.77605589 0.77592016\n",
      " 0.77678598 0.77558767 0.77692901 0.77618161 0.77656787 0.77708228]\n",
      "Best result :  0.7770822797242652\n",
      "Generation :  10\n",
      "0.7770822797242652\n",
      "0.7769416352539255\n",
      "0.7769290080161378\n",
      "0.7767859758499256\n",
      "0.7766587851274833\n",
      "0.7765678690154125\n",
      "0.7772513469589342\n",
      "0.7753200683358559\n",
      "0.7770159752465733\n",
      "0.7771207583615961\n",
      "0.7769581654561202\n",
      "0.7774214702898543\n",
      "[0.77708228 0.77694164 0.77692901 0.77678598 0.77665879 0.77656787\n",
      " 0.77725135 0.77532007 0.77701598 0.77712076 0.77695817 0.77742147]\n",
      "Best result :  0.7774214702898543\n",
      "Generation :  11\n",
      "0.7774214702898543\n",
      "0.7772513469589342\n",
      "0.7771207583615961\n",
      "0.7770822797242652\n",
      "0.7770159752465733\n",
      "0.7769581654561202\n",
      "0.733048484276885\n",
      "0.7314761865448012\n",
      "0.73223019335102\n",
      "0.7308209476966954\n",
      "0.7323365376518057\n",
      "0.7305682192720302\n",
      "[0.77742147 0.77725135 0.77712076 0.77708228 0.77701598 0.77695817\n",
      " 0.73304848 0.73147619 0.73223019 0.73082095 0.73233654 0.73056822]\n",
      "Best result :  0.7774214702898543\n",
      "Generation :  12\n",
      "0.7774214702898543\n",
      "0.7772513469589342\n",
      "0.7771207583615961\n",
      "0.7770822797242652\n",
      "0.7770159752465733\n",
      "0.7769581654561202\n",
      "0.7764581727569594\n",
      "0.7766300409425558\n",
      "0.7759185994069882\n",
      "0.7765085439564248\n",
      "0.7774562755489198\n",
      "0.7772058429856704\n",
      "[0.77742147 0.77725135 0.77712076 0.77708228 0.77701598 0.77695817\n",
      " 0.77645817 0.77663004 0.7759186  0.77650854 0.77745628 0.77720584]\n",
      "Best result :  0.7774562755489198\n",
      "Generation :  13\n",
      "0.7774562755489198\n",
      "0.7774214702898543\n",
      "0.7772513469589342\n",
      "0.7772058429856704\n",
      "0.7771207583615961\n",
      "0.7770822797242652\n",
      "0.7767953888817309\n",
      "0.776718431607069\n",
      "0.7754845438476929\n",
      "0.776622143179285\n",
      "0.7763888377421984\n",
      "0.776115630233703\n",
      "[0.77745628 0.77742147 0.77725135 0.77720584 0.77712076 0.77708228\n",
      " 0.77679539 0.77671843 0.77548454 0.77662214 0.77638884 0.77611563]\n",
      "Best result :  0.7774562755489198\n",
      "Generation :  14\n",
      "0.7774562755489198\n",
      "0.7774214702898543\n",
      "0.7772513469589342\n",
      "0.7772058429856704\n",
      "0.7771207583615961\n",
      "0.7770822797242652\n",
      "0.7802580989036894\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7801849068417497\n",
      "[0.77745628 0.77742147 0.77725135 0.77720584 0.77712076 0.77708228\n",
      " 0.7802581  0.78103135 0.78063412 0.78147119 0.78132324 0.78018491]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  15\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7802580989036894\n",
      "0.7801849068417497\n",
      "0.779883827575665\n",
      "0.7800955059982135\n",
      "0.7810945271347401\n",
      "0.780337030619169\n",
      "0.7804979245871972\n",
      "0.7804669763753105\n",
      "[0.78147119 0.78132324 0.78103135 0.78063412 0.7802581  0.78018491\n",
      " 0.77988383 0.78009551 0.78109453 0.78033703 0.78049792 0.78046698]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  16\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7287401625782121\n",
      "0.7302526760790251\n",
      "0.7253298900309932\n",
      "0.7305066901860833\n",
      "0.7262226127839625\n",
      "0.7262038785548085\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.72874016 0.73025268 0.72532989 0.73050669 0.72622261 0.72620388]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  17\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7667108635294975\n",
      "0.7674333251998617\n",
      "0.7673095323523148\n",
      "0.7680881967101408\n",
      "0.7667518676143861\n",
      "0.7661866265337847\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.76671086 0.76743333 0.76730953 0.7680882  0.76675187 0.76618663]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  18\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7721455348388245\n",
      "0.7726684861521446\n",
      "0.7719288514383893\n",
      "0.7720207318122547\n",
      "0.7722656543081059\n",
      "0.7715628452114621\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77214553 0.77266849 0.77192885 0.77202073 0.77226565 0.77156285]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  19\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7750265194952155\n",
      "0.7766817896588707\n",
      "0.7767435024137309\n",
      "0.7753097828767126\n",
      "0.7761240790037137\n",
      "0.7761312420913313\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77502652 0.77668179 0.7767435  0.77530978 0.77612408 0.77613124]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7728484357699248\n",
      "0.7739565103237082\n",
      "0.7715385549976815\n",
      "0.7721805237668034\n",
      "0.7719798195618228\n",
      "0.7725531420746083\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77284844 0.77395651 0.77153855 0.77218052 0.77197982 0.77255314]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  21\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7783290243075706\n",
      "0.7787442078860268\n",
      "0.7779445134376308\n",
      "0.7791363410158674\n",
      "0.7778335774140134\n",
      "0.7782067926457865\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77832902 0.77874421 0.77794451 0.77913634 0.77783358 0.77820679]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  22\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7722768581118157\n",
      "0.7735154294284803\n",
      "0.7709795127429033\n",
      "0.7715746918563684\n",
      "0.7721717535761945\n",
      "0.7727640399042754\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77227686 0.77351543 0.77097951 0.77157469 0.77217175 0.77276404]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  23\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.779160677146876\n",
      "0.7781759362683565\n",
      "0.779345585825315\n",
      "0.7786186701838039\n",
      "0.7783258560188165\n",
      "0.7776209806468986\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77916068 0.77817594 0.77934559 0.77861867 0.77832586 0.77762098]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  24\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7500353792244194\n",
      "0.7528408759574546\n",
      "0.7501937018276618\n",
      "0.7502321804649927\n",
      "0.751588759058436\n",
      "0.7481036873462863\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.75003538 0.75284088 0.7501937  0.75023218 0.75158876 0.74810369]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  25\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7603373226527411\n",
      "0.7608883753097919\n",
      "0.7603654239964721\n",
      "0.7607316138923125\n",
      "0.7596953998008478\n",
      "0.7600451054117219\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.76033732 0.76088838 0.76036542 0.76073161 0.7596954  0.76004511]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  26\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7656903531301177\n",
      "0.7671664083516458\n",
      "0.7670503755156849\n",
      "0.7669140472648072\n",
      "0.76517116702952\n",
      "0.7656936132533283\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.76569035 0.76716641 0.76705038 0.76691405 0.76517117 0.76569361]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  27\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7195944608761797\n",
      "0.7228519208875543\n",
      "0.7170271368064328\n",
      "0.7214183391022209\n",
      "0.7196551175347886\n",
      "0.7165940455089324\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.71959446 0.72285192 0.71702714 0.72141834 0.71965512 0.71659405]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  28\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7314485902905816\n",
      "0.7321275224284998\n",
      "0.7308415186149821\n",
      "0.7323340122042483\n",
      "0.7298589817634218\n",
      "0.7304705533273965\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.73144859 0.73212752 0.73084152 0.73233401 0.72985898 0.73047055]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  29\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7723469278022297\n",
      "0.7710054559769033\n",
      "0.7703797878238346\n",
      "0.7719864316427009\n",
      "0.7710566996037068\n",
      "0.7696440560744866\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77234693 0.77100546 0.77037979 0.77198643 0.7710567  0.76964406]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  30\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.769300916627262\n",
      "0.7705324166907656\n",
      "0.7701736653859128\n",
      "0.7692187247885719\n",
      "0.7693737872686035\n",
      "0.7716021962761311\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.76930092 0.77053242 0.77017367 0.76921872 0.76937379 0.7716022 ]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  31\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7707117693845777\n",
      "0.7707508449458769\n",
      "0.7718079972934548\n",
      "0.7722314918902369\n",
      "0.7704878769792965\n",
      "0.7705046826848612\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77071177 0.77075084 0.771808   0.77223149 0.77048788 0.77050468]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  32\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7747849948742598\n",
      "0.7768090262985414\n",
      "0.7769272172442335\n",
      "0.7758862736782519\n",
      "0.7772469848222439\n",
      "0.7755912554863053\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.77478499 0.77680903 0.77692722 0.77588627 0.77724698 0.77559126]\n",
      "Best result :  0.7814711861586372\n",
      "Generation :  33\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.78063411508639\n",
      "0.7804979245871972\n",
      "0.7807928968619156\n",
      "0.7819253075467076\n",
      "0.7808360590565351\n",
      "0.7809850604624287\n",
      "0.7806927054697246\n",
      "0.7804053554549071\n",
      "[0.78147119 0.78132324 0.78109453 0.78103135 0.78063412 0.78049792\n",
      " 0.7807929  0.78192531 0.78083606 0.78098506 0.78069271 0.78040536]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  34\n",
      "0.7819253075467076\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.7809850604624287\n",
      "0.7660512166274733\n",
      "0.7670193813865698\n",
      "0.7664152943308111\n",
      "0.7655475046328187\n",
      "0.7657118423929709\n",
      "0.7663123938221491\n",
      "[0.78192531 0.78147119 0.78132324 0.78109453 0.78103135 0.78098506\n",
      " 0.76605122 0.76701938 0.76641529 0.7655475  0.76571184 0.76631239]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  35\n",
      "0.7819253075467076\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.7809850604624287\n",
      "0.754367991136873\n",
      "0.7543722155218782\n",
      "0.7557129067543232\n",
      "0.7538092243854645\n",
      "0.7542737689843633\n",
      "0.753117986430357\n",
      "[0.78192531 0.78147119 0.78132324 0.78109453 0.78103135 0.78098506\n",
      " 0.75436799 0.75437222 0.75571291 0.75380922 0.75427377 0.75311799]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  36\n",
      "0.7819253075467076\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.7809850604624287\n",
      "0.7649324892767202\n",
      "0.7653001944410951\n",
      "0.7663873766559932\n",
      "0.7649910796600545\n",
      "0.7657908200256787\n",
      "0.7661388726163334\n",
      "[0.78192531 0.78147119 0.78132324 0.78109453 0.78103135 0.78098506\n",
      " 0.76493249 0.76530019 0.76638738 0.76499108 0.76579082 0.76613887]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  37\n",
      "0.7819253075467076\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.781031345028574\n",
      "0.7809850604624287\n",
      "0.7807103836026272\n",
      "0.7804158705001919\n",
      "0.7795465196164368\n",
      "0.7801246175209675\n",
      "0.7810944812175118\n",
      "0.7799424638762276\n",
      "[0.78192531 0.78147119 0.78132324 0.78109453 0.78103135 0.78098506\n",
      " 0.78071038 0.78041587 0.77954652 0.78012462 0.78109448 0.77994246]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  38\n",
      "0.7819253075467076\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.7810944812175118\n",
      "0.781031345028574\n",
      "0.7815333121685522\n",
      "0.7810709256793844\n",
      "0.7798664249461322\n",
      "0.7818933032385696\n",
      "0.7806532166533705\n",
      "0.780720301723944\n",
      "[0.78192531 0.78147119 0.78132324 0.78109453 0.78109448 0.78103135\n",
      " 0.78153331 0.78107093 0.77986642 0.7818933  0.78065322 0.7807203 ]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  39\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.7731727491535388\n",
      "0.7717031682612036\n",
      "0.7727838761469091\n",
      "0.7715628452114621\n",
      "0.771786967202885\n",
      "0.7734028403846431\n",
      "[0.78192531 0.7818933  0.78153331 0.78147119 0.78132324 0.78109453\n",
      " 0.77317275 0.77170317 0.77278388 0.77156285 0.77178697 0.77340284]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.7716566541089169\n",
      "0.7726825827412382\n",
      "0.7718000995301841\n",
      "0.7719263719080602\n",
      "0.7717272288888426\n",
      "0.7734594104099315\n",
      "[0.78192531 0.7818933  0.78153331 0.78147119 0.78132324 0.78109453\n",
      " 0.77165665 0.77268258 0.7718001  0.77192637 0.77172723 0.77345941]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  41\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.7754085049175974\n",
      "0.7760490961698694\n",
      "0.7762222041206304\n",
      "0.7750617380093358\n",
      "0.775823137489314\n",
      "0.7766043732119258\n",
      "[0.78192531 0.7818933  0.78153331 0.78147119 0.78132324 0.78109453\n",
      " 0.7754085  0.7760491  0.7762222  0.77506174 0.77582314 0.77660437]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  42\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.77845984249105\n",
      "0.7792968676460689\n",
      "0.7780211033744662\n",
      "0.7782731430407069\n",
      "0.7790656284842565\n",
      "0.7790208591866461\n",
      "[0.78192531 0.7818933  0.78153331 0.78147119 0.78132324 0.78109453\n",
      " 0.77845984 0.77929687 0.7780211  0.77827314 0.77906563 0.77902086]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  43\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810945271347401\n",
      "0.7810712011827544\n",
      "0.7799651010697888\n",
      "0.781551587225423\n",
      "0.7809947030803756\n",
      "0.7806272734193705\n",
      "0.7805606475210803\n",
      "[0.78192531 0.7818933  0.78153331 0.78147119 0.78132324 0.78109453\n",
      " 0.7810712  0.7799651  0.78155159 0.7809947  0.78062727 0.78056065]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  44\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.781551587225423\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7660757823446238\n",
      "0.7648996584584723\n",
      "0.7650681746864014\n",
      "0.7650681746864014\n",
      "0.7664607064696181\n",
      "0.7646671336142672\n",
      "[0.78192531 0.7818933  0.78155159 0.78153331 0.78147119 0.78132324\n",
      " 0.76607578 0.76489966 0.76506817 0.76506817 0.76646071 0.76466713]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  45\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.781551587225423\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7810898435774516\n",
      "0.7795841258264298\n",
      "0.7805019193860611\n",
      "0.7805019193860611\n",
      "0.7795916103346456\n",
      "0.77994622908895\n",
      "[0.78192531 0.7818933  0.78155159 0.78153331 0.78147119 0.78132324\n",
      " 0.78108984 0.77958413 0.78050192 0.78050192 0.77959161 0.77994623]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  46\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.781551587225423\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7702867595192614\n",
      "0.7702368934093075\n",
      "0.7698555049108935\n",
      "0.7698555049108935\n",
      "0.7709611458515759\n",
      "0.7705030296646417\n",
      "[0.78192531 0.7818933  0.78155159 0.78153331 0.78147119 0.78132324\n",
      " 0.77028676 0.77023689 0.7698555  0.7698555  0.77096115 0.77050303]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  47\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.781551587225423\n",
      "0.7815333121685522\n",
      "0.7814711861586372\n",
      "0.781323240848995\n",
      "0.7807720504402589\n",
      "0.780246941017208\n",
      "0.7817744694516813\n",
      "0.7817744694516813\n",
      "0.7797844626935836\n",
      "0.7816376820285202\n",
      "[0.78192531 0.7818933  0.78155159 0.78153331 0.78147119 0.78132324\n",
      " 0.78077205 0.78024694 0.78177447 0.78177447 0.77978446 0.78163768]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  48\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7817744694516813\n",
      "0.7817744694516813\n",
      "0.7816376820285202\n",
      "0.781551587225423\n",
      "0.776278360890864\n",
      "0.7768729430803609\n",
      "0.7770832439860598\n",
      "0.7767275691355044\n",
      "0.7763525172145985\n",
      "0.7776239193495108\n",
      "[0.78192531 0.7818933  0.78177447 0.78177447 0.78163768 0.78155159\n",
      " 0.77627836 0.77687294 0.77708324 0.77672757 0.77635252 0.77762392]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  49\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7817744694516813\n",
      "0.7817744694516813\n",
      "0.7816376820285202\n",
      "0.781551587225423\n",
      "0.7801224134940081\n",
      "0.7806536299084255\n",
      "0.7816133918147399\n",
      "0.7813040015303294\n",
      "0.780045272550433\n",
      "0.7799098626441217\n",
      "[0.78192531 0.7818933  0.78177447 0.78177447 0.78163768 0.78155159\n",
      " 0.78012241 0.78065363 0.78161339 0.781304   0.78004527 0.77990986]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  50\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7817744694516813\n",
      "0.7817744694516813\n",
      "0.7816376820285202\n",
      "0.7816133918147399\n",
      "0.7115232223168015\n",
      "0.7082124524033491\n",
      "0.7118454235079128\n",
      "0.7118512090786809\n",
      "0.7106682435255101\n",
      "0.7111757207328868\n",
      "[0.78192531 0.7818933  0.78177447 0.78177447 0.78163768 0.78161339\n",
      " 0.71152322 0.70821245 0.71184542 0.71185121 0.71066824 0.71117572]\n",
      "Best result :  0.7819253075467076\n",
      "Generation :  51\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7817744694516813\n",
      "0.7817744694516813\n",
      "0.7816376820285202\n",
      "0.7816133918147399\n",
      "0.7800421501789072\n",
      "0.7822324019697021\n",
      "0.7804552674820893\n",
      "0.7811978408984569\n",
      "0.78026163453027\n",
      "0.780129898002224\n",
      "[0.78192531 0.7818933  0.78177447 0.78177447 0.78163768 0.78161339\n",
      " 0.78004215 0.7822324  0.78045527 0.78119784 0.78026163 0.7801299 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  52\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7817744694516813\n",
      "0.7817744694516813\n",
      "0.7816376820285202\n",
      "0.7710716686201387\n",
      "0.7711932574407261\n",
      "0.7718254917574443\n",
      "0.771107484058227\n",
      "0.7699039935039979\n",
      "0.7709537990950449\n",
      "[0.7822324  0.78192531 0.7818933  0.78177447 0.78177447 0.78163768\n",
      " 0.77107167 0.77119326 0.77182549 0.77110748 0.76990399 0.7709538 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  53\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7817744694516813\n",
      "0.7817744694516813\n",
      "0.7816376820285202\n",
      "0.7806916034562449\n",
      "0.7806884351674909\n",
      "0.7808773386447934\n",
      "0.7818597836618972\n",
      "0.7818724568169131\n",
      "0.7796271961865926\n",
      "[0.7822324  0.78192531 0.7818933  0.78177447 0.78177447 0.78163768\n",
      " 0.7806916  0.78068844 0.78087734 0.78185978 0.78187246 0.7796272 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  54\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7545446806314426\n",
      "0.7565652682636005\n",
      "0.7549308445216018\n",
      "0.7553433649008156\n",
      "0.7555417273271517\n",
      "0.7561743289816963\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.75454468 0.75656527 0.75493084 0.75534336 0.75554173 0.75617433]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  55\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7423066372710692\n",
      "0.7429441979862724\n",
      "0.7400796516976196\n",
      "0.7398981408940764\n",
      "0.7395149157065297\n",
      "0.7420423836220961\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.74230664 0.7429442  0.74007965 0.73989814 0.73951492 0.74204238]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  56\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7644422310299629\n",
      "0.7660822107565883\n",
      "0.7640429807297332\n",
      "0.7647809624232689\n",
      "0.7652900926508649\n",
      "0.7662663388421457\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76444223 0.76608221 0.76404298 0.76478096 0.76529009 0.76626634]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  57\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7803808356549851\n",
      "0.7811716680783153\n",
      "0.7801222298250947\n",
      "0.7807944580476784\n",
      "0.7801233777558028\n",
      "0.7811489849675259\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.78038084 0.78117167 0.78012223 0.78079446 0.78012338 0.78114898]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  58\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7802578693175478\n",
      "0.7804699609951513\n",
      "0.7800707106949214\n",
      "0.7797629734307305\n",
      "0.7800796645544436\n",
      "0.7800770013552011\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.78025787 0.78046996 0.78007071 0.77976297 0.78007966 0.780077  ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  59\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7804840116670166\n",
      "0.7812365491219294\n",
      "0.7791691259168867\n",
      "0.7794786998702105\n",
      "0.7811663875970587\n",
      "0.7809982846241845\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.78048401 0.78123655 0.77916913 0.7794787  0.78116639 0.78099828]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7722783274631217\n",
      "0.7734121156647634\n",
      "0.7729703460111106\n",
      "0.7719861102221026\n",
      "0.7711722732673846\n",
      "0.7727643613248736\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77227833 0.77341212 0.77297035 0.77198611 0.77117227 0.77276436]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  61\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7499529118823594\n",
      "0.7504364202965537\n",
      "0.7487590180288488\n",
      "0.7493821148171316\n",
      "0.7498214967749116\n",
      "0.7497202952036975\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.74995291 0.75043642 0.74875902 0.74938211 0.7498215  0.7497203 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  62\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7714430012455507\n",
      "0.7719044693901522\n",
      "0.7700409186788437\n",
      "0.7708346898047865\n",
      "0.7704297916854737\n",
      "0.7714699087413454\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.771443   0.77190447 0.77004092 0.77083469 0.77042979 0.77146991]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  63\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7741929381323205\n",
      "0.7721332290216352\n",
      "0.7718524910876956\n",
      "0.7714759698154835\n",
      "0.7707219170920363\n",
      "0.7709992571510802\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77419294 0.77213323 0.77185249 0.77147597 0.77072192 0.77099926]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  64\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7551950522533466\n",
      "0.7556942643596261\n",
      "0.7542860288843243\n",
      "0.7541706388895597\n",
      "0.7537613786335565\n",
      "0.7551410995100724\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.75519505 0.75569426 0.75428603 0.75417064 0.75376138 0.7551411 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  65\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7763735473051685\n",
      "0.7760633764278766\n",
      "0.7761012581412394\n",
      "0.7770891213912847\n",
      "0.7774114603340809\n",
      "0.7763871388047506\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77637355 0.77606338 0.77610126 0.77708912 0.77741146 0.77638714]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  66\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7772815604951677\n",
      "0.77628556989571\n",
      "0.7770173068461945\n",
      "0.7765990468134407\n",
      "0.7773092026666155\n",
      "0.7765928020703894\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77728156 0.77628557 0.77701731 0.77659905 0.7773092  0.7765928 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  67\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7690175614113082\n",
      "0.7686125255403103\n",
      "0.7692032506826284\n",
      "0.7678416671112984\n",
      "0.7673381387855573\n",
      "0.7677131447892347\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76901756 0.76861253 0.76920325 0.76784167 0.76733814 0.76771314]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  68\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.773572458626052\n",
      "0.7722127576610829\n",
      "0.7719653556349025\n",
      "0.7717081273218621\n",
      "0.7730127276128487\n",
      "0.773049094057677\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77357246 0.77221276 0.77196536 0.77170813 0.77301273 0.77304909]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  69\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7730090542345832\n",
      "0.7715768958833277\n",
      "0.7718214969585805\n",
      "0.7723140510667537\n",
      "0.7720113646976778\n",
      "0.7730156203982328\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77300905 0.7715769  0.7718215  0.77231405 0.77201136 0.77301562]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  70\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7720331294639009\n",
      "0.7728988069693904\n",
      "0.7729924781151603\n",
      "0.7722760316017059\n",
      "0.7717132241342055\n",
      "0.7714002982232145\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77203313 0.77289881 0.77299248 0.77227603 0.77171322 0.7714003 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  71\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7608041171958275\n",
      "0.7612843654868112\n",
      "0.7600961653696121\n",
      "0.759666747450377\n",
      "0.7595503472765895\n",
      "0.7605657608636259\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76080412 0.76128437 0.76009617 0.75966675 0.75955035 0.76056576]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  72\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7785249990380342\n",
      "0.7792274407968514\n",
      "0.7785764263337508\n",
      "0.7784881275036943\n",
      "0.7773891445611181\n",
      "0.7795216324786882\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.778525   0.77922744 0.77857643 0.77848813 0.77738914 0.77952163]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  73\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7610120304056538\n",
      "0.76132918070165\n",
      "0.7604740641586737\n",
      "0.7604472025801075\n",
      "0.760002172803244\n",
      "0.7618445097550693\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76101203 0.76132918 0.76047406 0.7604472  0.76000217 0.76184451]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  74\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.780770167833898\n",
      "0.7800279617553568\n",
      "0.7805525201716679\n",
      "0.7812078967714586\n",
      "0.7805156945545565\n",
      "0.7800849909529284\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.78077017 0.78002796 0.78055252 0.7812079  0.78051569 0.78008499]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  75\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7709469115107971\n",
      "0.7699611604532545\n",
      "0.7701805529701605\n",
      "0.7699747978700651\n",
      "0.7705120753586205\n",
      "0.769652780347867\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77094691 0.76996116 0.77018055 0.7699748  0.77051208 0.76965278]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  76\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7722338336688812\n",
      "0.7722524760635785\n",
      "0.7726785879423747\n",
      "0.7717205708907364\n",
      "0.7719732533981734\n",
      "0.7727768508109764\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.77223383 0.77225248 0.77267859 0.77172057 0.77197325 0.77277685]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  77\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7426910563065523\n",
      "0.7427558455157096\n",
      "0.741012322439226\n",
      "0.7419512838411121\n",
      "0.7417408911009565\n",
      "0.7436420939394861\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.74269106 0.74275585 0.74101232 0.74195128 0.74174089 0.74364209]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  78\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7512320740288575\n",
      "0.7533145580847888\n",
      "0.7517417093459652\n",
      "0.7518900219934341\n",
      "0.7519751066175082\n",
      "0.7514977511119085\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.75123207 0.75331456 0.75174171 0.75189002 0.75197511 0.75149775]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  79\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7233375874114796\n",
      "0.7233310212478301\n",
      "0.7193086261298969\n",
      "0.7195983638405867\n",
      "0.7212278285219271\n",
      "0.7248119436955599\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.72333759 0.72333102 0.71930863 0.71959836 0.72122783 0.72481194]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7302534566719066\n",
      "0.7334530609755998\n",
      "0.7285821154783396\n",
      "0.7305022362149363\n",
      "0.7285019899149237\n",
      "0.7322068214818059\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.73025346 0.73345306 0.72858212 0.73050224 0.72850199 0.73220682]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  81\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.760085053400359\n",
      "0.7596902111540478\n",
      "0.759089154635358\n",
      "0.7602259274568404\n",
      "0.7607908930340719\n",
      "0.7602154583287839\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76008505 0.75969021 0.75908915 0.76022593 0.76079089 0.76021546]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  82\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7803522751389708\n",
      "0.7803478211678239\n",
      "0.7808788998305563\n",
      "0.7799924677378665\n",
      "0.7800124417321852\n",
      "0.7793644578061538\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.78035228 0.78034782 0.7808789  0.77999247 0.78001244 0.77936446]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  83\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7663435716201775\n",
      "0.7665411993708605\n",
      "0.7651985336975977\n",
      "0.7663540866654625\n",
      "0.7664137331450481\n",
      "0.7675114763224599\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76634357 0.7665412  0.76519853 0.76635409 0.76641373 0.76751148]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  84\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7668180802576213\n",
      "0.7682258106434116\n",
      "0.7679111857949728\n",
      "0.7672404728409237\n",
      "0.7667247764496781\n",
      "0.7679907603516488\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76681808 0.76822581 0.76791119 0.76724047 0.76672478 0.76799076]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  85\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7632896167647105\n",
      "0.764539116381715\n",
      "0.763804257059705\n",
      "0.7646512921704972\n",
      "0.7647731105772264\n",
      "0.7656525173339832\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.76328962 0.76453912 0.76380426 0.76465129 0.76477311 0.76565252]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  86\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7350833980842781\n",
      "0.7354884798725044\n",
      "0.7305715712296974\n",
      "0.7311785510708403\n",
      "0.7314379834108401\n",
      "0.733978813239847\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.7350834  0.73548848 0.73057157 0.73117855 0.73143798 0.73397881]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  87\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818597836618972\n",
      "0.7817744694516813\n",
      "0.7818649263914688\n",
      "0.781806427842591\n",
      "0.7796689808643624\n",
      "0.7815233940472355\n",
      "0.7805418673746982\n",
      "0.7818016983680742\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78185978 0.78177447\n",
      " 0.78186493 0.78180643 0.77966898 0.78152339 0.78054187 0.7818017 ]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  88\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7731065824275318\n",
      "0.773063098812314\n",
      "0.7731268778424486\n",
      "0.7727005822947393\n",
      "0.7737094297181261\n",
      "0.7719527283971149\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.77310658 0.7730631  0.77312688 0.77270058 0.77370943 0.77195273]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  89\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7707892776659795\n",
      "0.7713990584580499\n",
      "0.7707088306819654\n",
      "0.7700461073256437\n",
      "0.7704498115970204\n",
      "0.7704453117086452\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.77078928 0.77139906 0.77070883 0.77004611 0.77044981 0.77044531]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  90\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7579614734250827\n",
      "0.7582620476016558\n",
      "0.7561049939669353\n",
      "0.7582539202522435\n",
      "0.7600453809150918\n",
      "0.7580881131407852\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.75796147 0.75826205 0.75610499 0.75825392 0.76004538 0.75808811]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  91\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7803274798356787\n",
      "0.7810225289207368\n",
      "0.7813617194863258\n",
      "0.7804458085330557\n",
      "0.7809440563775403\n",
      "0.7800849909529284\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.78032748 0.78102253 0.78136172 0.78044581 0.78094406 0.78008499]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  92\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7800351248429747\n",
      "0.7810712011827544\n",
      "0.7808221920535827\n",
      "0.7805052254264999\n",
      "0.7806924758835829\n",
      "0.7801387600272895\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.78003512 0.7810712  0.78082219 0.78050523 0.78069248 0.78013876]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  93\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.771799778109586\n",
      "0.7720966329906652\n",
      "0.7722658838942476\n",
      "0.7721194538531397\n",
      "0.7721890643712706\n",
      "0.7723665344587218\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.77179978 0.77209663 0.77226588 0.77211945 0.77218906 0.77236653]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  94\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7537476952995176\n",
      "0.7556656120091552\n",
      "0.7527463323843466\n",
      "0.7540663608640485\n",
      "0.7542980591981437\n",
      "0.7538204741064024\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.7537477  0.75566561 0.75274633 0.75406636 0.75429806 0.75382047]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  95\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7776226795843463\n",
      "0.7778957952583849\n",
      "0.7760018014247014\n",
      "0.7765704403801984\n",
      "0.7766499231024176\n",
      "0.7773069527224279\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.77762268 0.7778958  0.7760018  0.77657044 0.77664992 0.77730695]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  96\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7516737059308253\n",
      "0.7541855160715351\n",
      "0.7514046768901069\n",
      "0.752645038978676\n",
      "0.751288506302461\n",
      "0.7533425216768347\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.75167371 0.75418552 0.75140468 0.75264504 0.75128851 0.75334252]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  97\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7723652028591005\n",
      "0.7735678209859917\n",
      "0.7712947804325384\n",
      "0.7732038351171105\n",
      "0.7720943371292494\n",
      "0.7718114870028072\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.7723652  0.77356782 0.77129478 0.77320384 0.77209434 0.77181149]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  98\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7663211640127581\n",
      "0.76775396520521\n",
      "0.7670926652829664\n",
      "0.7657194646528718\n",
      "0.7659660401689423\n",
      "0.7652016101518951\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.76632116 0.76775397 0.76709267 0.76571946 0.76596604 0.76520161]\n",
      "Best result :  0.7822324019697021\n",
      "Generation :  99\n",
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7811801627655542\n",
      "0.7813896371611435\n",
      "0.7810348806551544\n",
      "0.780602110778252\n",
      "0.7818113869032494\n",
      "0.7801833915732153\n",
      "[0.7822324  0.78192531 0.7818933  0.78187246 0.78186493 0.78185978\n",
      " 0.78118016 0.78138964 0.78103488 0.78060211 0.78181139 0.78018339]\n",
      "Best result :  0.7822324019697021\n"
     ]
    }
   ],
   "source": [
    "for generation in range(num_generations):\n",
    "    print(\"Generation : \", generation)\n",
    "    \n",
    "    # Measuring the fitness of each chromosome in the population.\n",
    "    fitness = ga.cal_pop_fitness(new_population, features, labels)\n",
    "\n",
    "    print( fitness )\n",
    "    \n",
    "    best_outputs.append(np.max(fitness))\n",
    "    # The best result in the current iteration.\n",
    "    print(\"Best result : \", best_outputs[-1])\n",
    "\n",
    "    # Selecting the best parents in the population for mating.\n",
    "    parents = ga.select_mating_pool(new_population, fitness, num_parents_mating)\n",
    "\n",
    "    # Generating next generation using crossover.\n",
    "    offspring_crossover = ga.crossover(parents, offspring_size=(pop_shape[0]-parents.shape[0], num_feature_elements))\n",
    "\n",
    "    # Adding some variations to the offspring using mutation.\n",
    "    offspring_mutation = ga.mutation(offspring_crossover, num_mutations=num_mutations)\n",
    "\n",
    "    # Creating the new population based on the parents and offspring.\n",
    "    new_population[0:parents.shape[0], :] = parents\n",
    "    new_population[parents.shape[0]:, :] = offspring_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7822324019697021\n",
      "0.7819253075467076\n",
      "0.7818933032385696\n",
      "0.7818724568169131\n",
      "0.7818649263914688\n",
      "0.7818597836618972\n",
      "0.7717151526577948\n",
      "0.7714571437518729\n",
      "0.7712103386496606\n",
      "0.7722144565985307\n",
      "0.7714556284833383\n",
      "0.7720901127442441\n",
      "best_match_idx :  0\n",
      "best_solution :  [1 0 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Selected indices :  [ 0  2  3  4  5  8  9 12 13 14 15 16 18 19 20 21 22 23 24 25 26 27]\n",
      "Number of selected elements :  22\n",
      "Best solution fitness :  0.7822324019697021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f74e890a518>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Getting the best solution after iterating finishing all generations.\n",
    "# At first, the fitness is calculated for each solution in the final generation.\n",
    "fitness = ga.cal_pop_fitness(new_population, features, labels)\n",
    "# Then return the index of that solution corresponding to the best fitness.\n",
    "best_match_idx = np.where(fitness == np.max(fitness))[0]\n",
    "best_match_idx = best_match_idx[0]\n",
    "\n",
    "best_solution = new_population[best_match_idx, :]\n",
    "best_solution_indices = np.where(best_solution == 1)[0]\n",
    "best_solution_num_elements = best_solution_indices.shape[0]\n",
    "best_solution_fitness = fitness[best_match_idx]\n",
    "\n",
    "print(\"best_match_idx : \", best_match_idx)\n",
    "print(\"best_solution : \", best_solution)\n",
    "print(\"Selected indices : \", best_solution_indices)\n",
    "print(\"Number of selected elements : \", best_solution_num_elements)\n",
    "print(\"Best solution fitness : \", best_solution_fitness)\n",
    "\n",
    "plt.plot(best_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSV_classifier = sklearn.svm.SVC(gamma='scale')\\nSV_classifier.fit(X=X_train, y=y_train)\\n\\npredictions = SV_classifier.decision_function(X_test)#.predict(X_test)\\nscore = roc_auc_score(y_test, predictions) # classification_accuracy(y_test, predictions)\\nprint(score)\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import sklearn.svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "\"\"\"\n",
    "SV_classifier = sklearn.svm.SVC(gamma='scale')\n",
    "SV_classifier.fit(X=X_train, y=y_train)\n",
    "\n",
    "predictions = SV_classifier.decision_function(X_test)#.predict(X_test)\n",
    "score = roc_auc_score(y_test, predictions) # classification_accuracy(y_test, predictions)\n",
    "print(score)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_380 (Dense)            (None, 64)                1856      \n",
      "_________________________________________________________________\n",
      "dense_381 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_382 (Dense)            (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_383 (Dense)            (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 10,241\n",
      "Trainable params: 10,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13400 samples, validate on 6600 samples\n",
      "Epoch 1/1000\n",
      " - 7s - loss: 0.6746 - val_loss: 0.6591\n",
      "Epoch 2/1000\n",
      " - 1s - loss: 0.6517 - val_loss: 0.6526\n",
      "Epoch 3/1000\n",
      " - 1s - loss: 0.6456 - val_loss: 0.6546\n",
      "Epoch 4/1000\n",
      " - 1s - loss: 0.6422 - val_loss: 0.6503\n",
      "Epoch 5/1000\n",
      " - 1s - loss: 0.6343 - val_loss: 0.6409\n",
      "Epoch 6/1000\n",
      " - 1s - loss: 0.6301 - val_loss: 0.6408\n",
      "Epoch 7/1000\n",
      " - 1s - loss: 0.6251 - val_loss: 0.6388\n",
      "Epoch 8/1000\n",
      " - 1s - loss: 0.6216 - val_loss: 0.6344\n",
      "Epoch 9/1000\n",
      " - 1s - loss: 0.6141 - val_loss: 0.6294\n",
      "Epoch 10/1000\n",
      " - 1s - loss: 0.6066 - val_loss: 0.6276\n",
      "Epoch 11/1000\n",
      " - 1s - loss: 0.5998 - val_loss: 0.6279\n",
      "Epoch 12/1000\n",
      " - 1s - loss: 0.5960 - val_loss: 0.6334\n",
      "Epoch 13/1000\n",
      " - 1s - loss: 0.5889 - val_loss: 0.6242\n",
      "Epoch 14/1000\n",
      " - 1s - loss: 0.5884 - val_loss: 0.6194\n",
      "Epoch 15/1000\n",
      " - 1s - loss: 0.5797 - val_loss: 0.6215\n",
      "Epoch 16/1000\n",
      " - 1s - loss: 0.5774 - val_loss: 0.6363\n",
      "Epoch 17/1000\n",
      " - 1s - loss: 0.5760 - val_loss: 0.6228\n",
      "Epoch 18/1000\n",
      " - 1s - loss: 0.5703 - val_loss: 0.6229\n",
      "Epoch 19/1000\n",
      " - 1s - loss: 0.5690 - val_loss: 0.6208\n",
      "Epoch 20/1000\n",
      " - 1s - loss: 0.5664 - val_loss: 0.6286\n",
      "Epoch 21/1000\n",
      " - 1s - loss: 0.5619 - val_loss: 0.6196\n",
      "Epoch 22/1000\n",
      " - 1s - loss: 0.5582 - val_loss: 0.6160\n",
      "Epoch 23/1000\n",
      " - 1s - loss: 0.5558 - val_loss: 0.6229\n",
      "Epoch 24/1000\n",
      " - 1s - loss: 0.5533 - val_loss: 0.6251\n",
      "Epoch 25/1000\n",
      " - 1s - loss: 0.5519 - val_loss: 0.6226\n",
      "Epoch 26/1000\n",
      " - 1s - loss: 0.5478 - val_loss: 0.6314\n",
      "Epoch 27/1000\n",
      " - 1s - loss: 0.5458 - val_loss: 0.6242\n",
      "Epoch 28/1000\n",
      " - 1s - loss: 0.5401 - val_loss: 0.6265\n",
      "Epoch 29/1000\n",
      " - 1s - loss: 0.5393 - val_loss: 0.6282\n",
      "Epoch 30/1000\n",
      " - 1s - loss: 0.5367 - val_loss: 0.6298\n",
      "Epoch 31/1000\n",
      " - 1s - loss: 0.5358 - val_loss: 0.6382\n",
      "Epoch 32/1000\n",
      " - 1s - loss: 0.5303 - val_loss: 0.6335\n",
      "0.7208931837619417\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "#from keras.initializers import TruncatedNormal\n",
    "#from keras import initializations\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape\n",
    "#from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='tanh', input_dim=features.shape[1]))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='tanh'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='tanh'))\n",
    "model.add(Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping\n",
    "patience = 10\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 100, \n",
    "                    epochs=1000, verbose=2, callbacks=[es])\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print( roc_auc_score(y_test, predictions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=4,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_rounds = 5\n",
    "\n",
    "# Define model\n",
    "model_bdt = xgb.XGBClassifier(n_jobs = 4)\n",
    "\n",
    "# Last in list is used for early stopping\n",
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "# Fit with early stopping\n",
    "model_bdt.fit(X_train, y_train, eval_metric=[\"logloss\"], eval_set=eval_set, \n",
    "              early_stopping_rounds=early_stopping_rounds, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f38ea47fb00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "plot_importance(model_bdt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_bdt.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7805039397441069\n"
     ]
    }
   ],
   "source": [
    "predictions = model_bdt.predict_proba(X_test)[:,1]\n",
    "score = roc_auc_score(y_test, predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore = model_bdt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore = list(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(list(data.drop(['label'], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance['f-score'] = fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance.columns = ['feature', 'fscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = feature_importance.sort_values(by=['fscore'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>m_bb</td>\n",
       "      <td>0.147746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>m_wbb</td>\n",
       "      <td>0.101843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>m_wwbb</td>\n",
       "      <td>0.073595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jet1_pt</td>\n",
       "      <td>0.069575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>m_jjj</td>\n",
       "      <td>0.068811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>m_jlv</td>\n",
       "      <td>0.050949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jet2_pt</td>\n",
       "      <td>0.046317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jet1_btag</td>\n",
       "      <td>0.045502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lepton_pT</td>\n",
       "      <td>0.035983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>missing_energy_magnitude</td>\n",
       "      <td>0.033924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>m_jj</td>\n",
       "      <td>0.031528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jet4_pt</td>\n",
       "      <td>0.027733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jet3_btag</td>\n",
       "      <td>0.025484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>m_lv</td>\n",
       "      <td>0.025395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jet3_pt</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jet1_phi</td>\n",
       "      <td>0.021146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jet4_eta</td>\n",
       "      <td>0.018537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lepton_phi</td>\n",
       "      <td>0.017199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jet2_phi</td>\n",
       "      <td>0.016399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lepton_eta</td>\n",
       "      <td>0.016094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jet3_eta</td>\n",
       "      <td>0.015550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jet1_eta</td>\n",
       "      <td>0.015540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jet4_btag</td>\n",
       "      <td>0.015194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jet4_phi</td>\n",
       "      <td>0.014956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jet2_eta</td>\n",
       "      <td>0.014521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>missing_energy_phi</td>\n",
       "      <td>0.013444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jet3_phi</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jet2_btag</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature    fscore\n",
       "25                      m_bb  0.147746\n",
       "26                     m_wbb  0.101843\n",
       "27                    m_wwbb  0.073595\n",
       "5                    jet1_pt  0.069575\n",
       "22                     m_jjj  0.068811\n",
       "24                     m_jlv  0.050949\n",
       "9                    jet2_pt  0.046317\n",
       "8                  jet1_btag  0.045502\n",
       "0                  lepton_pT  0.035983\n",
       "3   missing_energy_magnitude  0.033924\n",
       "21                      m_jj  0.031528\n",
       "17                   jet4_pt  0.027733\n",
       "16                 jet3_btag  0.025484\n",
       "23                      m_lv  0.025395\n",
       "13                   jet3_pt  0.024014\n",
       "7                   jet1_phi  0.021146\n",
       "18                  jet4_eta  0.018537\n",
       "2                 lepton_phi  0.017199\n",
       "11                  jet2_phi  0.016399\n",
       "1                 lepton_eta  0.016094\n",
       "14                  jet3_eta  0.015550\n",
       "6                   jet1_eta  0.015540\n",
       "20                 jet4_btag  0.015194\n",
       "19                  jet4_phi  0.014956\n",
       "10                  jet2_eta  0.014521\n",
       "4         missing_energy_phi  0.013444\n",
       "15                  jet3_phi  0.013021\n",
       "12                 jet2_btag  0.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feat = feature_importance.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m_bb', 'm_wbb', 'm_wwbb', 'jet1_pt', 'm_jjj']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reduced_feat['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness = pd.DataFrame(best_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness.to_hdf('xgb_sol_12_parents_6_mutations_3.h5', 'frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "# Skopt dimensions\n",
    "skopt_dims = [       \n",
    "    Real(        low=1e-2, high=1,     prior='log-uniform', name='learning_rate'     ),\n",
    "    Integer(        low=2,    high=20,                         name='max_depth'     ),\n",
    "    Integer(        low=1,    high=20,                         name='min_child_weight'     ),\n",
    "    Real(        low=1e-6, high=1e-2,     prior='log-uniform', name='reg_alpha'     ),\n",
    "    Integer(     low=5,    high=27,                        name='n_feat'       )\n",
    "\n",
    "]\n",
    "\n",
    "# Initial parameters\n",
    "init_param = {'learning_rate' : 0.3, 'reg_alpha' : 1e-5, 'max_depth' : 6, 'min_child_weight' : 1, 'n_feat':12 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model( n_features, dense_layers, dense_units, regulizer_value, dropout_value, learning_rate ):\n",
    "\n",
    "    m_input = Input(shape = (n_features, ))\n",
    "    m = m_input\n",
    "    \n",
    "    for _ in range(dense_layers):\n",
    "        m = Dense( units=dense_units, activation='relu', \n",
    "                   kernel_initializer='lecun_normal',\n",
    "                   kernel_regularizer=keras.regularizers.l2(regulizer_value) )(m)\n",
    "        m = Dropout(dropout_value)(m)\n",
    "\n",
    "    m_output = Dense( units=1, activation='sigmoid', \n",
    "                      kernel_initializer='lecun_normal',\n",
    "                      kernel_regularizer=keras.regularizers.l2(regulizer_value) )(m)\n",
    "\n",
    "    model = keras.models.Model(inputs=m_input, outputs=m_output)\n",
    "    model.compile( loss = 'binary_crossentropy',\n",
    "                        optimizer = keras.optimizers.Adam(lr=learning_rate) )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(features.shape[1], 3, 50, 0., 0., 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                1450      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 6,601\n",
      "Trainable params: 6,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sol = [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "red_f = ga.reduce_features(np.array(best_sol), features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\n",
    "    Integer(     low=1,    high=5,                        name='dense_layers'      ),\n",
    "    #Integer(     low=100,    high=1000,                    name='batch_size'      ),\n",
    "    Integer(     low=5,    high=200,                        name='dense_units'       ),\n",
    "    #Real(        low=1e-3, high=0.9,  prior=\"log-uniform\", name='regulizer_value'   ),\n",
    "    Real(        low=1e-3, high=0.5,   prior=\"log-uniform\",                    name='dropout_value'     ),\n",
    "    Real(        low=1e-4, high=1e-1, prior='log-uniform', name='learning_rate'     ),\n",
    "    Integer(     low=5,    high=27,                        name='n_feat'       )\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Initial parameters\n",
    "init_param = {'learning_rate' : 1e-2, 'dense_layers' : 3, 'regulizer_value' : 1e-2, 'dropout_value': 0.02, \n",
    "             'dense_units' : 20, 'batch_size' : 100, 'n_feat': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import skopt\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def optimize( algo, dimensions, initial_param, data, num_calls=12, random_seed = 1): \n",
    "\n",
    "    prior_values = []\n",
    "    prior_names = []\n",
    "    for var in dimensions:\n",
    "        name = var.name\n",
    "        print( name )\n",
    "        prior_names.append(name)\n",
    "        prior_values.append(initial_param[name])\n",
    "\n",
    "    global num_skopt_call\n",
    "    num_skopt_call = 0\n",
    "    #errors = []\n",
    "\n",
    "    @use_named_args(dimensions)\n",
    "    def fitness(**p): \n",
    "\n",
    "        global num_skopt_call\n",
    "\n",
    "        print('\\n \\t ::: {} SKOPT CALL ::: \\n'.format(num_skopt_call+1))\n",
    "        print(p)\n",
    "\n",
    "        reduced_feat = feature_importance.iloc[0:p['n_feat']]\n",
    "        reduced_feat = list(reduced_feat['feature'])\n",
    "        data_red = data[reduced_feat]\n",
    "        features = data_red.values\n",
    "        labels = data[['label']].values.ravel()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "        \n",
    "        if algo == 'xgboost':\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stopping_rounds = 10\n",
    "\n",
    "            # Define model\n",
    "            model_bdt = xgb.XGBClassifier(n_jobs = 4, n_estimators = 1000, learning_rate = p['learning_rate'],\n",
    "                                  max_depth = p['max_depth'], min_child_weight = p['min_child_weight'])\n",
    "\n",
    "            # Last in list is used for early stopping\n",
    "            eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "            # Fit with early stopping\n",
    "            model_bdt.fit(X_train, y_train, eval_metric=[\"logloss\"], eval_set=eval_set, \n",
    "                          early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "            predictions = model_bdt.predict_proba(X_test)[:,1]\n",
    "            score = roc_auc_score(y_test, predictions)\n",
    "            \n",
    "        if algo == 'keras':\n",
    "            \n",
    "            # Early stopping\n",
    "            patience = 5\n",
    "            \n",
    "            es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=patience)\n",
    "            \n",
    "            # Define model\n",
    "            model = create_model(features.shape[1], p['dense_layers'], p['dense_units'], 0., \n",
    "                                 #p['regulizer_value'], \n",
    "                                 p['dropout_value'], \n",
    "                                 p['learning_rate'])\n",
    "            \n",
    "            history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size = 100,# p['batch_size'], \n",
    "                                epochs=1000, verbose=0, callbacks=[es])\n",
    "\n",
    "            predictions = model.predict(X_test)\n",
    "            score = roc_auc_score(y_test, predictions)\n",
    "            \n",
    "            print(roc_auc_score(y_train, model.predict(X_train)))\n",
    "\n",
    "        print(score)\n",
    "        \n",
    "        num_skopt_call += 1\n",
    "\n",
    "        return -1*score\n",
    "\n",
    "    search_result = gp_minimize( func = fitness, dimensions = dimensions,\n",
    "                                 acq_func = 'EI', # Expected Improvement\n",
    "                                 n_calls = num_calls, x0 = prior_values )\n",
    "\n",
    "    params = pd.DataFrame(search_result['x_iters'])\n",
    "    params.columns = [*prior_names]\n",
    "    params = params.rename_axis('call').reset_index()\n",
    "    scores = pd.DataFrame(search_result['func_vals'])\n",
    "    scores.columns = ['score']\n",
    "    result = pd.concat([params, scores], axis=1)\n",
    "    result = result.sort_values(by=['score'])\n",
    "    #errors_frame = pd.DataFrame(errors, columns = ['call', 'q_error', 't_error'])\n",
    "    #result = pd.merge(result, errors_frame, on=['call'])   \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_layers\n",
      "dense_units\n",
      "dropout_value\n",
      "learning_rate\n",
      "n_feat\n",
      "\n",
      " \t ::: 1 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 20, 'dropout_value': 0.02, 'learning_rate': 0.01, 'n_feat': 20}\n",
      "0.7522726589731832\n",
      "0.7249038240694254\n",
      "\n",
      " \t ::: 2 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 94, 'dropout_value': 0.1309972517417965, 'learning_rate': 0.00023824750247923064, 'n_feat': 9}\n",
      "0.7516867602710045\n",
      "0.7333596194159714\n",
      "\n",
      " \t ::: 3 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 4, 'dense_units': 177, 'dropout_value': 0.3600555199575265, 'learning_rate': 0.0012344047856970145, 'n_feat': 10}\n",
      "0.808205395427435\n",
      "0.7696048427615025\n",
      "\n",
      " \t ::: 4 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 76, 'dropout_value': 0.012869296735372895, 'learning_rate': 0.03889574418918643, 'n_feat': 27}\n",
      "0.720811677492109\n",
      "0.704918166774863\n",
      "\n",
      " \t ::: 5 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 2, 'dense_units': 55, 'dropout_value': 0.14709907539029735, 'learning_rate': 0.0006494656473860387, 'n_feat': 13}\n",
      "0.7883235571421598\n",
      "0.7545223648584799\n",
      "\n",
      " \t ::: 6 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 2, 'dense_units': 118, 'dropout_value': 0.0014239713319435454, 'learning_rate': 0.09390010610940876, 'n_feat': 25}\n",
      "0.5\n",
      "0.5\n",
      "\n",
      " \t ::: 7 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 4, 'dense_units': 124, 'dropout_value': 0.25658173585807204, 'learning_rate': 0.00648333385628775, 'n_feat': 5}\n",
      "0.7496531066665785\n",
      "0.7349394016562711\n",
      "\n",
      " \t ::: 8 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 48, 'dropout_value': 0.008701138235154383, 'learning_rate': 0.049791429926308056, 'n_feat': 13}\n",
      "0.7092931614067441\n",
      "0.699985278936601\n",
      "\n",
      " \t ::: 9 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 174, 'dropout_value': 0.01222760085874564, 'learning_rate': 0.0033435462578784094, 'n_feat': 9}\n",
      "0.763463501287694\n",
      "0.7378224985051647\n",
      "\n",
      " \t ::: 10 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 170, 'dropout_value': 0.001022326884942001, 'learning_rate': 0.043252180882983285, 'n_feat': 12}\n",
      "0.5\n",
      "0.5\n",
      "\n",
      " \t ::: 11 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 151, 'dropout_value': 0.0035788562296684232, 'learning_rate': 0.00247145004060214, 'n_feat': 12}\n",
      "0.8229808671621581\n",
      "0.7549966898270104\n",
      "\n",
      " \t ::: 12 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 32, 'dropout_value': 0.003014065518596498, 'learning_rate': 0.0001, 'n_feat': 6}\n",
      "0.770845227909037\n",
      "0.7517260056538801\n",
      "\n",
      " \t ::: 13 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 37, 'dropout_value': 0.49999999999999994, 'learning_rate': 0.0001, 'n_feat': 23}\n",
      "0.5216203426756112\n",
      "0.527022564368837\n",
      "\n",
      " \t ::: 14 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 116, 'dropout_value': 0.02035957623838169, 'learning_rate': 0.00016447769640234313, 'n_feat': 12}\n",
      "0.7715789017121039\n",
      "0.7461644191254807\n",
      "\n",
      " \t ::: 15 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 2, 'dense_units': 87, 'dropout_value': 0.004387246689181639, 'learning_rate': 0.0005351883222790826, 'n_feat': 24}\n",
      "0.7884810561944366\n",
      "0.707517081897693\n",
      "\n",
      " \t ::: 16 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 200, 'dropout_value': 0.023229445770671923, 'learning_rate': 0.0012164115106664116, 'n_feat': 5}\n",
      "0.7562665742969255\n",
      "0.7404208626212547\n",
      "\n",
      " \t ::: 17 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 2, 'dense_units': 190, 'dropout_value': 0.001, 'learning_rate': 0.0001, 'n_feat': 7}\n",
      "0.7692505667125553\n",
      "0.7440996591196805\n",
      "\n",
      " \t ::: 18 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 4, 'dense_units': 78, 'dropout_value': 0.09692308640210819, 'learning_rate': 0.1, 'n_feat': 5}\n",
      "0.5\n",
      "0.5\n",
      "\n",
      " \t ::: 19 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 19, 'dropout_value': 0.2045257041029672, 'learning_rate': 0.010648147215740316, 'n_feat': 5}\n",
      "0.7455177429365518\n",
      "0.7317643171524999\n",
      "\n",
      " \t ::: 20 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 44, 'dropout_value': 0.49999999999999994, 'learning_rate': 0.0025542097936703404, 'n_feat': 6}\n",
      "0.7459992623163201\n",
      "0.7332514843432812\n",
      "\n",
      " \t ::: 21 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 200, 'dropout_value': 0.001, 'learning_rate': 0.004876368633175283, 'n_feat': 26}\n",
      "0.8050557819539459\n",
      "0.7018940581178033\n",
      "\n",
      " \t ::: 22 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 2, 'dense_units': 195, 'dropout_value': 0.09605225674537188, 'learning_rate': 0.002871470247817056, 'n_feat': 9}\n",
      "0.7926451128211156\n",
      "0.7612578712460715\n",
      "\n",
      " \t ::: 23 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 5, 'dropout_value': 0.49999999999999994, 'learning_rate': 0.013015184481876434, 'n_feat': 23}\n",
      "0.6505399443994928\n",
      "0.654848230244411\n",
      "\n",
      " \t ::: 24 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 5, 'dropout_value': 0.012151597802063627, 'learning_rate': 0.011324162238402794, 'n_feat': 5}\n",
      "0.7334788116658634\n",
      "0.7228879659117844\n",
      "\n",
      " \t ::: 25 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 35, 'dropout_value': 0.050939423797577064, 'learning_rate': 0.0010753356157181108, 'n_feat': 5}\n",
      "0.7788327689543036\n",
      "0.7573248769716743\n",
      "\n",
      " \t ::: 26 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 5, 'dropout_value': 0.001, 'learning_rate': 0.0009767364337916263, 'n_feat': 5}\n",
      "0.7312542545071963\n",
      "0.7180917736622268\n",
      "\n",
      " \t ::: 27 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 5, 'dropout_value': 0.20841045218253473, 'learning_rate': 0.0016165493338860105, 'n_feat': 5}\n",
      "0.7237591486175983\n",
      "0.7114003478138211\n",
      "\n",
      " \t ::: 28 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 200, 'dropout_value': 0.08265242966705964, 'learning_rate': 0.0005487211101823015, 'n_feat': 9}\n",
      "0.7725069320189314\n",
      "0.7467363182041882\n",
      "\n",
      " \t ::: 29 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 200, 'dropout_value': 0.4769162772007489, 'learning_rate': 0.0015940735673263544, 'n_feat': 27}\n",
      "0.784594327106908\n",
      "0.7088567170338866\n",
      "\n",
      " \t ::: 30 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 71, 'dropout_value': 0.001360840293112531, 'learning_rate': 0.004984713220183197, 'n_feat': 20}\n",
      "0.7921114873192434\n",
      "0.728995875622718\n",
      "\n",
      " \t ::: 31 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 95, 'dropout_value': 0.001, 'learning_rate': 0.0002427887438296209, 'n_feat': 5}\n",
      "0.7462594921860755\n",
      "0.7319169919366592\n",
      "\n",
      " \t ::: 32 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 122, 'dropout_value': 0.14169373729399937, 'learning_rate': 0.0014824088348257473, 'n_feat': 5}\n",
      "0.7527224891872\n",
      "0.7363727997726914\n",
      "\n",
      " \t ::: 33 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 88, 'dropout_value': 0.010790646067590517, 'learning_rate': 0.0001, 'n_feat': 5}\n",
      "0.758304560796071\n",
      "0.7395750213583988\n",
      "\n",
      " \t ::: 34 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 200, 'dropout_value': 0.0037396660682702124, 'learning_rate': 0.0011598317117430062, 'n_feat': 14}\n",
      "0.789290394146168\n",
      "0.7486116237259461\n",
      "\n",
      " \t ::: 35 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 4, 'dense_units': 200, 'dropout_value': 0.45800433922007133, 'learning_rate': 0.00033326021104481107, 'n_feat': 5}\n",
      "0.7675066489884562\n",
      "0.7491116164251067\n",
      "\n",
      " \t ::: 36 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 200, 'dropout_value': 0.49999999999999994, 'learning_rate': 0.000817029067770343, 'n_feat': 5}\n",
      "0.7662732220044488\n",
      "0.7493537838872588\n",
      "\n",
      " \t ::: 37 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 14, 'dropout_value': 0.001, 'learning_rate': 0.00011071108578004652, 'n_feat': 9}\n",
      "0.7500705682634864\n",
      "0.7333047024109024\n",
      "\n",
      " \t ::: 38 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 5, 'dropout_value': 0.001, 'learning_rate': 0.0003204737759349543, 'n_feat': 10}\n",
      "0.7325733146392874\n",
      "0.7232442836035362\n",
      "\n",
      " \t ::: 39 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 200, 'dropout_value': 0.09190499897261972, 'learning_rate': 0.0017284939569466602, 'n_feat': 15}\n",
      "0.7796510177435045\n",
      "0.7421910636073916\n",
      "\n",
      " \t ::: 40 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 10, 'dropout_value': 0.3829172389154952, 'learning_rate': 0.0012900412140215915, 'n_feat': 27}\n",
      "0.7230767232685291\n",
      "0.7034270507024464\n",
      "\n",
      " \t ::: 41 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 200, 'dropout_value': 0.06722434956543955, 'learning_rate': 0.006093489617285474, 'n_feat': 5}\n",
      "0.7582461279793636\n",
      "0.7410261435249499\n",
      "\n",
      " \t ::: 42 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 200, 'dropout_value': 0.027804918856282543, 'learning_rate': 0.0003445056613198617, 'n_feat': 5}\n",
      "0.7873946803057728\n",
      "0.7585797948216203\n",
      "\n",
      " \t ::: 43 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 200, 'dropout_value': 0.49999999999999994, 'learning_rate': 0.0011071103741269617, 'n_feat': 14}\n",
      "0.7787854412687927\n",
      "0.7527820559879785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t ::: 44 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 182, 'dropout_value': 0.0015945996561980752, 'learning_rate': 0.0001729465350937616, 'n_feat': 5}\n",
      "0.7797433228811437\n",
      "0.754661034888002\n",
      "\n",
      " \t ::: 45 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 199, 'dropout_value': 0.49999999999999994, 'learning_rate': 0.005523151422616881, 'n_feat': 11}\n",
      "0.7487977331007694\n",
      "0.7342003179492558\n",
      "\n",
      " \t ::: 46 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 1, 'dense_units': 200, 'dropout_value': 0.006253664120377052, 'learning_rate': 0.0003415760249905515, 'n_feat': 5}\n",
      "0.750256192146527\n",
      "0.7338841319150542\n",
      "\n",
      " \t ::: 47 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 5, 'dense_units': 41, 'dropout_value': 0.0013769505932758404, 'learning_rate': 0.00908647361094458, 'n_feat': 27}\n",
      "0.7790834196737791\n",
      "0.7227290923018022\n",
      "\n",
      " \t ::: 48 SKOPT CALL ::: \n",
      "\n",
      "{'dense_layers': 3, 'dense_units': 5, 'dropout_value': 0.00409590592228761, 'learning_rate': 0.005566641169909135, 'n_feat': 27}\n"
     ]
    }
   ],
   "source": [
    "search_result = optimize('keras', dimensions, init_param, data, \n",
    "                             num_calls=100, random_seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>dense_layers</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dense_units</th>\n",
       "      <th>dropout_value</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>997</td>\n",
       "      <td>185</td>\n",
       "      <td>0.254892</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>-0.757535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>202</td>\n",
       "      <td>56</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>-0.757025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>886</td>\n",
       "      <td>148</td>\n",
       "      <td>0.397118</td>\n",
       "      <td>0.004024</td>\n",
       "      <td>-0.754311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>125</td>\n",
       "      <td>197</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-0.753788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>-0.753774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>817</td>\n",
       "      <td>96</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>0.023510</td>\n",
       "      <td>-0.753619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>860</td>\n",
       "      <td>136</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.011773</td>\n",
       "      <td>-0.752556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>143</td>\n",
       "      <td>0.086635</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>-0.752156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>584</td>\n",
       "      <td>200</td>\n",
       "      <td>0.157366</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>-0.750641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>709</td>\n",
       "      <td>194</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>-0.750382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>-0.750227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>-0.750093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>410</td>\n",
       "      <td>146</td>\n",
       "      <td>0.056214</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>-0.749219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>320</td>\n",
       "      <td>177</td>\n",
       "      <td>0.004312</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.748864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>297</td>\n",
       "      <td>173</td>\n",
       "      <td>0.110525</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>-0.747681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>910</td>\n",
       "      <td>120</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>-0.747293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>254</td>\n",
       "      <td>139</td>\n",
       "      <td>0.024629</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>-0.746969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>65</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>-0.745543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>963</td>\n",
       "      <td>159</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>-0.745432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>199</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>-0.745108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>610</td>\n",
       "      <td>121</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>-0.744762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>138</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>-0.743882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.743616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.743595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>479</td>\n",
       "      <td>119</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.742905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>860</td>\n",
       "      <td>126</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.016924</td>\n",
       "      <td>-0.742673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>740</td>\n",
       "      <td>186</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.014260</td>\n",
       "      <td>-0.742154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>930</td>\n",
       "      <td>175</td>\n",
       "      <td>0.025530</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>-0.740029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>111</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>-0.739969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>628</td>\n",
       "      <td>21</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>-0.739270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>992</td>\n",
       "      <td>23</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>-0.738902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>-0.738752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.004070</td>\n",
       "      <td>0.015437</td>\n",
       "      <td>-0.738741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>151</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.737746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>-0.734274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>-0.733089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>379</td>\n",
       "      <td>5</td>\n",
       "      <td>0.209406</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>-0.730714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>-0.727725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.038464</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>-0.726867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>908</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.023832</td>\n",
       "      <td>-0.722257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>14</td>\n",
       "      <td>0.079443</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.718778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.717379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>410</td>\n",
       "      <td>20</td>\n",
       "      <td>0.054793</td>\n",
       "      <td>0.067624</td>\n",
       "      <td>-0.706272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.701374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>-0.680711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.022108</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>345</td>\n",
       "      <td>51</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>92</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>122</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.060052</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    call  dense_layers  batch_size  dense_units  dropout_value  learning_rate  \\\n",
       "45    45             5         997          185       0.254892       0.003022   \n",
       "40    40             1         202           56       0.004240       0.006782   \n",
       "46    46             4         886          148       0.397118       0.004024   \n",
       "32    32             3         125          197       0.001270       0.001100   \n",
       "49    49             3        1000          200       0.001000       0.004324   \n",
       "6      6             3         817           96       0.005326       0.023510   \n",
       "28    28             1         860          136       0.001000       0.011773   \n",
       "7      7             4         164          143       0.086635       0.001386   \n",
       "38    38             4         584          200       0.157366       0.002522   \n",
       "4      4             2         709          194       0.005772       0.000556   \n",
       "39    39             2         100          200       0.500000       0.001320   \n",
       "41    41             3         100          200       0.001000       0.001824   \n",
       "5      5             5         410          146       0.056214       0.000170   \n",
       "48    48             5         320          177       0.004312       0.000100   \n",
       "23    23             5         297          173       0.110525       0.000602   \n",
       "20    20             2         910          120       0.006949       0.002367   \n",
       "8      8             3         254          139       0.024629       0.000458   \n",
       "19    19             2        1000           65       0.004504       0.009151   \n",
       "10    10             3         963          159       0.001226       0.028129   \n",
       "33    33             1         266          199       0.001024       0.002152   \n",
       "1      1             1         610          121       0.001123       0.000832   \n",
       "22    22             1         225          138       0.003518       0.000239   \n",
       "0      0             3         100           20       0.020000       0.010000   \n",
       "26    26             1         100          110       0.500000       0.000380   \n",
       "9      9             3         479          119       0.006627       0.000151   \n",
       "30    30             3         860          126       0.020508       0.016924   \n",
       "24    24             5         740          186       0.001717       0.014260   \n",
       "21    21             5         930          175       0.025530       0.003907   \n",
       "31    31             5        1000          111       0.001000       0.009268   \n",
       "34    34             5         628           21       0.005233       0.001469   \n",
       "3      3             4         992           23       0.005972       0.000219   \n",
       "43    43             1        1000          200       0.500000       0.007347   \n",
       "15    15             2        1000          200       0.004070       0.015437   \n",
       "17    17             2        1000          151       0.001000       0.000100   \n",
       "35    35             5         100          200       0.500000       0.000315   \n",
       "37    37             4        1000            5       0.001259       0.000768   \n",
       "25    25             2         379            5       0.209406       0.004397   \n",
       "29    29             2         100            5       0.001000       0.000193   \n",
       "47    47             5         100            5       0.038464       0.002869   \n",
       "27    27             1         908            5       0.001271       0.023832   \n",
       "16    16             4         100           14       0.079443       0.000100   \n",
       "36    36             1        1000          200       0.500000       0.000100   \n",
       "2      2             1         410           20       0.054793       0.067624   \n",
       "12    12             1         100          200       0.002707       0.100000   \n",
       "44    44             1         100            5       0.001000       0.008200   \n",
       "42    42             4        1000          200       0.001000       0.022108   \n",
       "13    13             5         345           51       0.001000       0.100000   \n",
       "14    14             2         112           92       0.001377       0.100000   \n",
       "11    11             5        1000          200       0.500000       0.100000   \n",
       "18    18             3         100          122       0.006292       0.060052   \n",
       "\n",
       "       score  \n",
       "45 -0.757535  \n",
       "40 -0.757025  \n",
       "46 -0.754311  \n",
       "32 -0.753788  \n",
       "49 -0.753774  \n",
       "6  -0.753619  \n",
       "28 -0.752556  \n",
       "7  -0.752156  \n",
       "38 -0.750641  \n",
       "4  -0.750382  \n",
       "39 -0.750227  \n",
       "41 -0.750093  \n",
       "5  -0.749219  \n",
       "48 -0.748864  \n",
       "23 -0.747681  \n",
       "20 -0.747293  \n",
       "8  -0.746969  \n",
       "19 -0.745543  \n",
       "10 -0.745432  \n",
       "33 -0.745108  \n",
       "1  -0.744762  \n",
       "22 -0.743882  \n",
       "0  -0.743616  \n",
       "26 -0.743595  \n",
       "9  -0.742905  \n",
       "30 -0.742673  \n",
       "24 -0.742154  \n",
       "21 -0.740029  \n",
       "31 -0.739969  \n",
       "34 -0.739270  \n",
       "3  -0.738902  \n",
       "43 -0.738752  \n",
       "15 -0.738741  \n",
       "17 -0.737746  \n",
       "35 -0.734274  \n",
       "37 -0.733089  \n",
       "25 -0.730714  \n",
       "29 -0.727725  \n",
       "47 -0.726867  \n",
       "27 -0.722257  \n",
       "16 -0.718778  \n",
       "36 -0.717379  \n",
       "2  -0.706272  \n",
       "12 -0.701374  \n",
       "44 -0.680711  \n",
       "42 -0.500000  \n",
       "13 -0.500000  \n",
       "14 -0.500000  \n",
       "11 -0.500000  \n",
       "18 -0.500000  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate\n",
      "max_depth\n",
      "min_child_weight\n",
      "reg_alpha\n",
      "n_feat\n",
      "\n",
      " \t ::: 1 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.3, 'max_depth': 6, 'min_child_weight': 1, 'reg_alpha': 1e-05, 'n_feat': 12}\n",
      "0.7803278471735051\n",
      "\n",
      " \t ::: 2 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.14920754588937857, 'max_depth': 13, 'min_child_weight': 2, 'reg_alpha': 1.094889367465407e-05, 'n_feat': 14}\n",
      "0.773509643857712\n",
      "\n",
      " \t ::: 3 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.20529431551726032, 'max_depth': 4, 'min_child_weight': 13, 'reg_alpha': 1.2868518737029452e-06, 'n_feat': 16}\n",
      "0.7868276145109094\n",
      "\n",
      " \t ::: 4 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.07829001430056709, 'max_depth': 9, 'min_child_weight': 20, 'reg_alpha': 0.0004195834682074532, 'n_feat': 15}\n",
      "0.791731344909189\n",
      "\n",
      " \t ::: 5 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.10972742639318686, 'max_depth': 15, 'min_child_weight': 6, 'reg_alpha': 1.632316373651068e-06, 'n_feat': 17}\n",
      "0.7839214212961717\n",
      "\n",
      " \t ::: 6 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.6730446025175298, 'max_depth': 4, 'min_child_weight': 4, 'reg_alpha': 0.0002868354649666903, 'n_feat': 9}\n",
      "0.7642382207845436\n",
      "\n",
      " \t ::: 7 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.32389029296815625, 'max_depth': 15, 'min_child_weight': 11, 'reg_alpha': 0.000449204502170815, 'n_feat': 23}\n",
      "0.7733954017936554\n",
      "\n",
      " \t ::: 8 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.4843313137841277, 'max_depth': 18, 'min_child_weight': 13, 'reg_alpha': 4.949408110957889e-06, 'n_feat': 26}\n",
      "0.7652054212818457\n",
      "\n",
      " \t ::: 9 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.023379511784345188, 'max_depth': 4, 'min_child_weight': 8, 'reg_alpha': 0.00032836619412626353, 'n_feat': 17}\n",
      "0.7888925122683945\n",
      "\n",
      " \t ::: 10 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.141367914954703, 'max_depth': 17, 'min_child_weight': 19, 'reg_alpha': 0.00010617456594161195, 'n_feat': 21}\n",
      "0.7877078936950044\n",
      "\n",
      " \t ::: 11 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.059468502146187235, 'max_depth': 19, 'min_child_weight': 2, 'reg_alpha': 7.15192362910031e-05, 'n_feat': 19}\n",
      "0.7752836559737992\n",
      "\n",
      " \t ::: 12 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 5}\n",
      "0.7472029749955895\n",
      "\n",
      " \t ::: 13 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010744395062947847, 'max_depth': 2, 'min_child_weight': 20, 'reg_alpha': 0.0007813156304400706, 'n_feat': 18}\n",
      "0.7652637820790384\n",
      "\n",
      " \t ::: 14 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.06997645511858731, 'max_depth': 2, 'min_child_weight': 20, 'reg_alpha': 0.01, 'n_feat': 27}\n",
      "0.7784590618981686\n",
      "\n",
      " \t ::: 15 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.9991677510661359, 'max_depth': 19, 'min_child_weight': 12, 'reg_alpha': 0.0054880563986438105, 'n_feat': 27}\n",
      "0.7270084402294061\n",
      "\n",
      " \t ::: 16 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.23277363088931807, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 5}\n",
      "0.75312418525618\n",
      "\n",
      " \t ::: 17 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.06593720527446117, 'max_depth': 2, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 27}\n",
      "0.7762050769944677\n",
      "\n",
      " \t ::: 18 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.09476325321106445, 'max_depth': 2, 'min_child_weight': 3, 'reg_alpha': 1.4945193466487448e-06, 'n_feat': 18}\n",
      "0.7768766623758547\n",
      "\n",
      " \t ::: 19 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'min_child_weight': 1, 'reg_alpha': 1e-06, 'n_feat': 27}\n",
      "0.7644369964659345\n",
      "\n",
      " \t ::: 20 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01898777150718535, 'max_depth': 9, 'min_child_weight': 20, 'reg_alpha': 0.01, 'n_feat': 27}\n",
      "0.7909986436969099\n",
      "\n",
      " \t ::: 21 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 0.0005181313171090595, 'n_feat': 27}\n",
      "0.791316344999646\n",
      "\n",
      " \t ::: 22 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.02305056449798513, 'max_depth': 13, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 27}\n",
      "0.7910648563401452\n",
      "\n",
      " \t ::: 23 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01, 'max_depth': 15, 'min_child_weight': 1, 'reg_alpha': 0.01, 'n_feat': 27}\n",
      "0.7738697267621861\n",
      "\n",
      " \t ::: 24 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.04281947859515604, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 0.01, 'n_feat': 27}\n",
      "0.7901526646823689\n",
      "\n",
      " \t ::: 25 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.05522541725961297, 'max_depth': 2, 'min_child_weight': 20, 'reg_alpha': 0.01, 'n_feat': 5}\n",
      "0.7523835862806302\n",
      "\n",
      " \t ::: 26 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 1.0, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 0.007022477401956513, 'n_feat': 15}\n",
      "0.7431232751767147\n",
      "\n",
      " \t ::: 27 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 16}\n",
      "0.7933019437038251\n",
      "\n",
      " \t ::: 28 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03038952488558188, 'max_depth': 14, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 20}\n",
      "0.7910488771446904\n",
      "\n",
      " \t ::: 29 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01, 'max_depth': 20, 'min_child_weight': 1, 'reg_alpha': 0.000557788484101871, 'n_feat': 5}\n",
      "0.7253307624583313\n",
      "\n",
      " \t ::: 30 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03008120264586299, 'max_depth': 16, 'min_child_weight': 20, 'reg_alpha': 1.6961925391857455e-06, 'n_feat': 21}\n",
      "0.7931420599148199\n",
      "\n",
      " \t ::: 31 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 1.5985847132413676e-05, 'n_feat': 23}\n",
      "0.7928965864122288\n",
      "\n",
      " \t ::: 32 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.04635788328450247, 'max_depth': 12, 'min_child_weight': 13, 'reg_alpha': 5.668137344562252e-06, 'n_feat': 22}\n",
      "0.7918073379220563\n",
      "\n",
      " \t ::: 33 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.5163937648751108, 'max_depth': 2, 'min_child_weight': 3, 'reg_alpha': 0.0026303036945291357, 'n_feat': 26}\n",
      "0.7714023185812604\n",
      "\n",
      " \t ::: 34 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.02056140083502047, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 0.00020639320326797754, 'n_feat': 21}\n",
      "0.7918635406095181\n",
      "\n",
      " \t ::: 35 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.07288721803810962, 'max_depth': 12, 'min_child_weight': 20, 'reg_alpha': 3.750642591566778e-06, 'n_feat': 19}\n",
      "0.7902671822497954\n",
      "\n",
      " \t ::: 36 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.8896123299023585, 'max_depth': 2, 'min_child_weight': 19, 'reg_alpha': 1.623550911460644e-06, 'n_feat': 20}\n",
      "0.7646409148768973\n",
      "\n",
      " \t ::: 37 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.011106407842580962, 'max_depth': 16, 'min_child_weight': 20, 'reg_alpha': 3.100409045332566e-06, 'n_feat': 22}\n",
      "0.7926982239858928\n",
      "\n",
      " \t ::: 38 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.09053360041336256, 'max_depth': 7, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 24}\n",
      "0.7887607757403488\n",
      "\n",
      " \t ::: 39 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.05640511507770269, 'max_depth': 10, 'min_child_weight': 5, 'reg_alpha': 0.0032689290115288983, 'n_feat': 27}\n",
      "0.7893919998780439\n",
      "\n",
      " \t ::: 40 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010801001215348603, 'max_depth': 20, 'min_child_weight': 13, 'reg_alpha': 1.92350721617452e-06, 'n_feat': 27}\n",
      "0.7908462903333489\n",
      "\n",
      " \t ::: 41 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.034533860154644394, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 1e-06, 'n_feat': 25}\n",
      "0.7908471168434588\n",
      "\n",
      " \t ::: 42 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.05199556274665723, 'max_depth': 10, 'min_child_weight': 12, 'reg_alpha': 0.0024746413736076627, 'n_feat': 21}\n",
      "0.7919360898302614\n",
      "\n",
      " \t ::: 43 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01021678111916966, 'max_depth': 14, 'min_child_weight': 18, 'reg_alpha': 0.0011769038416398821, 'n_feat': 27}\n",
      "0.7912105057883717\n",
      "\n",
      " \t ::: 44 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.024742540884316336, 'max_depth': 15, 'min_child_weight': 15, 'reg_alpha': 0.004279732881232018, 'n_feat': 21}\n",
      "0.7940288134281077\n",
      "\n",
      " \t ::: 45 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.052132676331372745, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 0.007425832808772584, 'n_feat': 18}\n",
      "0.790910574452995\n",
      "\n",
      " \t ::: 46 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0427118918510663, 'max_depth': 13, 'min_child_weight': 12, 'reg_alpha': 0.0012934410692460898, 'n_feat': 27}\n",
      "0.7894350702382067\n",
      "\n",
      " \t ::: 47 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010662312004404184, 'max_depth': 16, 'min_child_weight': 20, 'reg_alpha': 4.7638453282430046e-06, 'n_feat': 16}\n",
      "0.793845925107715\n",
      "\n",
      " \t ::: 48 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010060146014785817, 'max_depth': 14, 'min_child_weight': 13, 'reg_alpha': 0.00043587557882472265, 'n_feat': 19}\n",
      "0.7926039559161547\n",
      "\n",
      " \t ::: 49 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.019593838187141003, 'max_depth': 15, 'min_child_weight': 20, 'reg_alpha': 0.009365232935893336, 'n_feat': 13}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7943578562862385\n",
      "\n",
      " \t ::: 50 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.012677125947329839, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 0.0002833277344813105, 'n_feat': 6}\n",
      "0.7704661581303018\n",
      "\n",
      " \t ::: 51 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.032122220088552145, 'max_depth': 10, 'min_child_weight': 12, 'reg_alpha': 4.855685799839368e-06, 'n_feat': 14}\n",
      "0.7931888036532482\n",
      "\n",
      " \t ::: 52 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010743561605877312, 'max_depth': 2, 'min_child_weight': 1, 'reg_alpha': 0.00021147143434025224, 'n_feat': 6}\n",
      "0.7572923216567966\n",
      "\n",
      " \t ::: 53 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010408536282612424, 'max_depth': 11, 'min_child_weight': 20, 'reg_alpha': 0.0001388956598448272, 'n_feat': 12}\n",
      "0.7938304510017715\n",
      "\n",
      " \t ::: 54 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010159832034262161, 'max_depth': 9, 'min_child_weight': 13, 'reg_alpha': 0.00039002896226887754, 'n_feat': 21}\n",
      "0.7918139500029341\n",
      "\n",
      " \t ::: 55 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.029028637415767458, 'max_depth': 8, 'min_child_weight': 1, 'reg_alpha': 9.64225727597333e-06, 'n_feat': 17}\n",
      "0.7883034860635243\n",
      "\n",
      " \t ::: 56 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.29349337667521597, 'max_depth': 9, 'min_child_weight': 20, 'reg_alpha': 1.435532781272924e-06, 'n_feat': 27}\n",
      "0.779541927893605\n",
      "\n",
      " \t ::: 57 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.012680739336224305, 'max_depth': 10, 'min_child_weight': 20, 'reg_alpha': 0.0006058941969049735, 'n_feat': 17}\n",
      "0.7916560406547466\n",
      "\n",
      " \t ::: 58 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0858737149057226, 'max_depth': 11, 'min_child_weight': 20, 'reg_alpha': 0.0012028060403286697, 'n_feat': 5}\n",
      "0.7575252597560564\n",
      "\n",
      " \t ::: 59 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010121480127306267, 'max_depth': 12, 'min_child_weight': 12, 'reg_alpha': 0.00021223241945867068, 'n_feat': 14}\n",
      "0.7941114644390812\n",
      "\n",
      " \t ::: 60 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010730255324823646, 'max_depth': 9, 'min_child_weight': 4, 'reg_alpha': 0.007216514835507007, 'n_feat': 15}\n",
      "0.7886853337342212\n",
      "\n",
      " \t ::: 61 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.15119060179563867, 'max_depth': 8, 'min_child_weight': 1, 'reg_alpha': 1.6689237872749802e-06, 'n_feat': 27}\n",
      "0.7818532174982477\n",
      "\n",
      " \t ::: 62 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.13072929292143826, 'max_depth': 8, 'min_child_weight': 11, 'reg_alpha': 0.005323900301387947, 'n_feat': 21}\n",
      "0.7894766712470632\n",
      "\n",
      " \t ::: 63 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.08054211481354139, 'max_depth': 20, 'min_child_weight': 3, 'reg_alpha': 7.490801202438535e-05, 'n_feat': 27}\n",
      "0.7757903066710661\n",
      "\n",
      " \t ::: 64 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03834072561747796, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 4.940684182652456e-05, 'n_feat': 15}\n",
      "0.7933151219483524\n",
      "\n",
      " \t ::: 65 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.07973707035612305, 'max_depth': 14, 'min_child_weight': 20, 'reg_alpha': 0.004227479436624406, 'n_feat': 27}\n",
      "0.7889458680877007\n",
      "\n",
      " \t ::: 66 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.020685605856615132, 'max_depth': 14, 'min_child_weight': 20, 'reg_alpha': 0.005456821567658066, 'n_feat': 15}\n",
      "0.7937347135807273\n",
      "\n",
      " \t ::: 67 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.011502127385782454, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 1.3666728490666488e-06, 'n_feat': 15}\n",
      "0.7943016535987765\n",
      "\n",
      " \t ::: 68 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.033280409115862386, 'max_depth': 8, 'min_child_weight': 12, 'reg_alpha': 1.1274049931789798e-06, 'n_feat': 16}\n",
      "0.7916458929472883\n",
      "\n",
      " \t ::: 69 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.9960837233404466, 'max_depth': 2, 'min_child_weight': 18, 'reg_alpha': 0.004926268139586389, 'n_feat': 5}\n",
      "0.7461730056471763\n",
      "\n",
      " \t ::: 70 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010727237403753195, 'max_depth': 19, 'min_child_weight': 10, 'reg_alpha': 0.0032562279925003814, 'n_feat': 19}\n",
      "0.7900440704373956\n",
      "\n",
      " \t ::: 71 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.02229098627495941, 'max_depth': 14, 'min_child_weight': 13, 'reg_alpha': 0.0014433766134833968, 'n_feat': 15}\n",
      "0.7962618141584671\n",
      "\n",
      " \t ::: 72 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.7793974071999588, 'max_depth': 2, 'min_child_weight': 1, 'reg_alpha': 0.00010678518327523704, 'n_feat': 19}\n",
      "0.7703113711536401\n",
      "\n",
      " \t ::: 73 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.783761749737515, 'max_depth': 18, 'min_child_weight': 1, 'reg_alpha': 0.001826949172698194, 'n_feat': 7}\n",
      "0.7028747123630025\n",
      "\n",
      " \t ::: 74 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.09906392498393404, 'max_depth': 15, 'min_child_weight': 20, 'reg_alpha': 0.0008467294568318471, 'n_feat': 14}\n",
      "0.7913759455620035\n",
      "\n",
      " \t ::: 75 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03941355793221192, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 2.381449581925724e-06, 'n_feat': 13}\n",
      "0.7941113266873961\n",
      "\n",
      " \t ::: 76 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.011945271331444778, 'max_depth': 7, 'min_child_weight': 20, 'reg_alpha': 0.002071430505044583, 'n_feat': 27}\n",
      "0.7906048575468501\n",
      "\n",
      " \t ::: 77 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010191209970943919, 'max_depth': 11, 'min_child_weight': 1, 'reg_alpha': 1.3859306523158844e-06, 'n_feat': 18}\n",
      "0.7821046602405199\n",
      "\n",
      " \t ::: 78 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.12015399036014072, 'max_depth': 2, 'min_child_weight': 1, 'reg_alpha': 0.00017923957264683258, 'n_feat': 11}\n",
      "0.7760949674809597\n",
      "\n",
      " \t ::: 79 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010712809645256442, 'max_depth': 8, 'min_child_weight': 13, 'reg_alpha': 1.4751462421954445e-06, 'n_feat': 27}\n",
      "0.789975378263831\n",
      "\n",
      " \t ::: 80 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.5704905566669644, 'max_depth': 10, 'min_child_weight': 20, 'reg_alpha': 8.318243582950887e-05, 'n_feat': 14}\n",
      "0.7770602394546722\n",
      "\n",
      " \t ::: 81 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03426846813688822, 'max_depth': 8, 'min_child_weight': 10, 'reg_alpha': 1.198407188756764e-06, 'n_feat': 12}\n",
      "0.7909208139949099\n",
      "\n",
      " \t ::: 82 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.030468863433775143, 'max_depth': 15, 'min_child_weight': 15, 'reg_alpha': 1.0052225974293564e-06, 'n_feat': 15}\n",
      "0.7944529049488578\n",
      "\n",
      " \t ::: 83 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03595783575938799, 'max_depth': 8, 'min_child_weight': 11, 'reg_alpha': 2.6501693019982003e-06, 'n_feat': 27}\n",
      "0.7908378415633383\n",
      "\n",
      " \t ::: 84 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01051777496030048, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 6.685978582540666e-05, 'n_feat': 12}\n",
      "0.7945801415885284\n",
      "\n",
      " \t ::: 85 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010151370627945138, 'max_depth': 15, 'min_child_weight': 19, 'reg_alpha': 2.683246629803663e-06, 'n_feat': 12}\n",
      "0.7949527598963336\n",
      "\n",
      " \t ::: 86 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010180691521678872, 'max_depth': 15, 'min_child_weight': 20, 'reg_alpha': 0.0013618163526992124, 'n_feat': 17}\n",
      "0.7924961422640628\n",
      "\n",
      " \t ::: 87 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.011761516048364997, 'max_depth': 20, 'min_child_weight': 10, 'reg_alpha': 6.708874352803414e-06, 'n_feat': 15}\n",
      "0.7944676443791481\n",
      "\n",
      " \t ::: 88 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010354297419777512, 'max_depth': 20, 'min_child_weight': 5, 'reg_alpha': 3.882450047951077e-06, 'n_feat': 22}\n",
      "0.7850345926622984\n",
      "\n",
      " \t ::: 89 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010022292813055287, 'max_depth': 20, 'min_child_weight': 18, 'reg_alpha': 0.0010588343731589706, 'n_feat': 17}\n",
      "0.7923685382865655\n",
      "\n",
      " \t ::: 90 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.013261257839209693, 'max_depth': 14, 'min_child_weight': 11, 'reg_alpha': 1.428765795390144e-06, 'n_feat': 13}\n",
      "0.7924467812436204\n",
      "\n",
      " \t ::: 91 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010162219602212032, 'max_depth': 2, 'min_child_weight': 3, 'reg_alpha': 0.00011224416439166148, 'n_feat': 13}\n",
      "0.76457529915763\n",
      "\n",
      " \t ::: 92 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.0308146768212499, 'max_depth': 20, 'min_child_weight': 13, 'reg_alpha': 1.7854582533768466e-06, 'n_feat': 22}\n",
      "0.7929515952517545\n",
      "\n",
      " \t ::: 93 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.10143553770884903, 'max_depth': 8, 'min_child_weight': 7, 'reg_alpha': 0.006146766754214813, 'n_feat': 17}\n",
      "0.7878273703230891\n",
      "\n",
      " \t ::: 94 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.04097374823707612, 'max_depth': 20, 'min_child_weight': 15, 'reg_alpha': 1.7586826444630934e-06, 'n_feat': 14}\n",
      "0.7944961589779338\n",
      "\n",
      " \t ::: 95 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.08089086406247388, 'max_depth': 20, 'min_child_weight': 19, 'reg_alpha': 1.7051942543259848e-06, 'n_feat': 27}\n",
      "0.7902553356048893\n",
      "\n",
      " \t ::: 96 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.07804447327556299, 'max_depth': 17, 'min_child_weight': 20, 'reg_alpha': 1.0002257378048438e-06, 'n_feat': 15}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7942381959892404\n",
      "\n",
      " \t ::: 97 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.01816610557344866, 'max_depth': 15, 'min_child_weight': 20, 'reg_alpha': 1.128740266015245e-06, 'n_feat': 12}\n",
      "0.7946682108324437\n",
      "\n",
      " \t ::: 98 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.010585039324484588, 'max_depth': 18, 'min_child_weight': 20, 'reg_alpha': 0.009657127755839667, 'n_feat': 13}\n",
      "0.7944222322403411\n",
      "\n",
      " \t ::: 99 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.05564404346620192, 'max_depth': 20, 'min_child_weight': 20, 'reg_alpha': 1.252939036744693e-06, 'n_feat': 17}\n",
      "0.7924796120618682\n",
      "\n",
      " \t ::: 100 SKOPT CALL ::: \n",
      "\n",
      "{'learning_rate': 0.03198397376970495, 'max_depth': 16, 'min_child_weight': 13, 'reg_alpha': 5.679771822991072e-06, 'n_feat': 27}\n",
      "0.7887255572262283\n"
     ]
    }
   ],
   "source": [
    "search_result = optimize('xgboost', skopt_dims, init_param, data, \n",
    "                             num_calls=100, random_seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>n_feat</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>0.022291</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.796262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.794953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.794668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>0.010518</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.794580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>0.040974</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.794496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.794468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>0.030469</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.794453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.794422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.019594</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.009365</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.794358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.794302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>0.078044</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.794238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.794111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.794111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.024743</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.794029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.793846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.793830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.793735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.038341</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.793315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.793302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.793189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.030081</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.793142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.792952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.792897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.011106</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.792698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.792604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>0.010181</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.792496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.055644</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.792480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>0.013261</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.792447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.792369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.051996</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.791936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.293493</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.779542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.069976</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.778459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>0.570491</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.777060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.094763</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.776877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.065937</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.776205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>0.120154</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.776095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.080542</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.775790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.059469</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.775284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.773870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.149208</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.773510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.323890</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.773395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.516394</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.771402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.770466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>0.779397</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.770311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.765264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.484331</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.765205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.889612</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.764641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.764575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.764437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.673045</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.764238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.085874</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.757525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.757292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.232774</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.753124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.055225</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.752384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.747203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>0.996084</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.746173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.007022</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.743123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.999168</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.727008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.725331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>0.783762</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.702875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    call  learning_rate  max_depth  min_child_weight  reg_alpha  n_feat  \\\n",
       "70    70       0.022291         14                13   0.001443      15   \n",
       "84    84       0.010151         15                19   0.000003      12   \n",
       "96    96       0.018166         15                20   0.000001      12   \n",
       "83    83       0.010518         20                20   0.000067      12   \n",
       "93    93       0.040974         20                15   0.000002      14   \n",
       "86    86       0.011762         20                10   0.000007      15   \n",
       "81    81       0.030469         15                15   0.000001      15   \n",
       "97    97       0.010585         18                20   0.009657      13   \n",
       "48    48       0.019594         15                20   0.009365      13   \n",
       "66    66       0.011502         20                20   0.000001      15   \n",
       "95    95       0.078044         17                20   0.000001      15   \n",
       "58    58       0.010121         12                12   0.000212      14   \n",
       "74    74       0.039414         20                20   0.000002      13   \n",
       "43    43       0.024743         15                15   0.004280      21   \n",
       "46    46       0.010662         16                20   0.000005      16   \n",
       "52    52       0.010409         11                20   0.000139      12   \n",
       "65    65       0.020686         14                20   0.005457      15   \n",
       "63    63       0.038341         20                20   0.000049      15   \n",
       "26    26       0.010000         20                20   0.000001      16   \n",
       "50    50       0.032122         10                12   0.000005      14   \n",
       "29    29       0.030081         16                20   0.000002      21   \n",
       "91    91       0.030815         20                13   0.000002      22   \n",
       "30    30       0.010000         20                20   0.000016      23   \n",
       "36    36       0.011106         16                20   0.000003      22   \n",
       "47    47       0.010060         14                13   0.000436      19   \n",
       "85    85       0.010181         15                20   0.001362      17   \n",
       "98    98       0.055644         20                20   0.000001      17   \n",
       "89    89       0.013261         14                11   0.000001      13   \n",
       "88    88       0.010022         20                18   0.001059      17   \n",
       "41    41       0.051996         10                12   0.002475      21   \n",
       "..   ...            ...        ...               ...        ...     ...   \n",
       "55    55       0.293493          9                20   0.000001      27   \n",
       "13    13       0.069976          2                20   0.010000      27   \n",
       "79    79       0.570491         10                20   0.000083      14   \n",
       "17    17       0.094763          2                 3   0.000001      18   \n",
       "16    16       0.065937          2                20   0.000001      27   \n",
       "77    77       0.120154          2                 1   0.000179      11   \n",
       "62    62       0.080542         20                 3   0.000075      27   \n",
       "10    10       0.059469         19                 2   0.000072      19   \n",
       "22    22       0.010000         15                 1   0.010000      27   \n",
       "1      1       0.149208         13                 2   0.000011      14   \n",
       "6      6       0.323890         15                11   0.000449      23   \n",
       "32    32       0.516394          2                 3   0.002630      26   \n",
       "49    49       0.012677         20                20   0.000283       6   \n",
       "71    71       0.779397          2                 1   0.000107      19   \n",
       "12    12       0.010744          2                20   0.000781      18   \n",
       "7      7       0.484331         18                13   0.000005      26   \n",
       "35    35       0.889612          2                19   0.000002      20   \n",
       "90    90       0.010162          2                 3   0.000112      13   \n",
       "18    18       0.010000          2                 1   0.000001      27   \n",
       "5      5       0.673045          4                 4   0.000287       9   \n",
       "57    57       0.085874         11                20   0.001203       5   \n",
       "51    51       0.010744          2                 1   0.000211       6   \n",
       "15    15       0.232774         20                20   0.000001       5   \n",
       "24    24       0.055225          2                20   0.010000       5   \n",
       "11    11       0.010000          2                20   0.000001       5   \n",
       "68    68       0.996084          2                18   0.004926       5   \n",
       "25    25       1.000000         20                20   0.007022      15   \n",
       "14    14       0.999168         19                12   0.005488      27   \n",
       "28    28       0.010000         20                 1   0.000558       5   \n",
       "72    72       0.783762         18                 1   0.001827       7   \n",
       "\n",
       "       score  \n",
       "70 -0.796262  \n",
       "84 -0.794953  \n",
       "96 -0.794668  \n",
       "83 -0.794580  \n",
       "93 -0.794496  \n",
       "86 -0.794468  \n",
       "81 -0.794453  \n",
       "97 -0.794422  \n",
       "48 -0.794358  \n",
       "66 -0.794302  \n",
       "95 -0.794238  \n",
       "58 -0.794111  \n",
       "74 -0.794111  \n",
       "43 -0.794029  \n",
       "46 -0.793846  \n",
       "52 -0.793830  \n",
       "65 -0.793735  \n",
       "63 -0.793315  \n",
       "26 -0.793302  \n",
       "50 -0.793189  \n",
       "29 -0.793142  \n",
       "91 -0.792952  \n",
       "30 -0.792897  \n",
       "36 -0.792698  \n",
       "47 -0.792604  \n",
       "85 -0.792496  \n",
       "98 -0.792480  \n",
       "89 -0.792447  \n",
       "88 -0.792369  \n",
       "41 -0.791936  \n",
       "..       ...  \n",
       "55 -0.779542  \n",
       "13 -0.778459  \n",
       "79 -0.777060  \n",
       "17 -0.776877  \n",
       "16 -0.776205  \n",
       "77 -0.776095  \n",
       "62 -0.775790  \n",
       "10 -0.775284  \n",
       "22 -0.773870  \n",
       "1  -0.773510  \n",
       "6  -0.773395  \n",
       "32 -0.771402  \n",
       "49 -0.770466  \n",
       "71 -0.770311  \n",
       "12 -0.765264  \n",
       "7  -0.765205  \n",
       "35 -0.764641  \n",
       "90 -0.764575  \n",
       "18 -0.764437  \n",
       "5  -0.764238  \n",
       "57 -0.757525  \n",
       "51 -0.757292  \n",
       "15 -0.753124  \n",
       "24 -0.752384  \n",
       "11 -0.747203  \n",
       "68 -0.746173  \n",
       "25 -0.743123  \n",
       "14 -0.727008  \n",
       "28 -0.725331  \n",
       "72 -0.702875  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result.to_hdf('keras_all.h5', 'frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
